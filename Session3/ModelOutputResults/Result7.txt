from keras.layers import Activation
model = Sequential()

 
model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))
model.add(Convolution2D(16, 3, 3, activation='relu'))
model.add(Convolution2D(32, 3, 3, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(8, 1, 1, activation='relu'))
model.add(Convolution2D(16, 3, 3, activation='relu'))
model.add(Convolution2D(32, 3, 3, activation='relu'))

model.add(Convolution2D(8, 1, 1))
model.add(Convolution2D(16, 3, 3))
model.add(Convolution2D(24, 3, 3))

model.add(Convolution2D(10, 3, 3))

model.add(Flatten())
model.add(Activation('softmax'))

model.summary()

=================================================================
Total params: 19,042
Trainable params: 19,042
Non-trainable params: 0


/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  after removing the cwd from sys.path.
Epoch 1/10
60000/60000 [==============================] - 16s 265us/step - loss: 0.1940 - acc: 0.9396
Epoch 2/10
60000/60000 [==============================] - 14s 230us/step - loss: 0.0767 - acc: 0.9762
Epoch 3/10
60000/60000 [==============================] - 14s 231us/step - loss: 0.0598 - acc: 0.9816
Epoch 4/10
60000/60000 [==============================] - 14s 231us/step - loss: 0.0484 - acc: 0.9849
Epoch 5/10
60000/60000 [==============================] - 14s 230us/step - loss: 0.0431 - acc: 0.9865
Epoch 6/10
60000/60000 [==============================] - 14s 231us/step - loss: 0.0372 - acc: 0.9882
Epoch 7/10
60000/60000 [==============================] - 14s 232us/step - loss: 0.0347 - acc: 0.9896
Epoch 8/10
60000/60000 [==============================] - 14s 230us/step - loss: 0.0325 - acc: 0.9900
Epoch 9/10
60000/60000 [==============================] - 14s 231us/step - loss: 0.0294 - acc: 0.9903
Epoch 10/10
60000/60000 [==============================] - 14s 231us/step - loss: 0.0281 - acc: 0.9912
[0.030809315282250463, 0.9899]

