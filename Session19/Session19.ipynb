{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cpZk0SJcUf",
        "colab_type": "text"
      },
      "source": [
        "## Assignment19:\n",
        "\n",
        "1. Train any GAN to create 64x64 images of Indian Cars.\n",
        "  Ref: https://github.com/simontomaskarlsson/CycleGAN-Keras\n",
        "2. You have a team of 4 people, divide the project between yourselves including image collection [1000 Images], model selection, etc. \n",
        "3. The deadline ends next to next week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMoMDN4quqN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'https://github.com/arjuntheprogrammer/TheSchoolOfAI/raw/master/Session19/carImages.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AwDGFwXu3ci",
        "colab_type": "code",
        "outputId": "e671a0aa-97e0-4514-b576-c257c4c924ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name = \"/content/carImages.zip\"\n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEZ2UrCwytOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import os \n",
        "import shutil\n",
        "\n",
        "def resize_images(input_image_path, output_image_path):\n",
        "  ori_img = cv2.imread(input_image_path,cv2.IMREAD_COLOR)\n",
        "  try:\n",
        "    new_img = cv2.resize(ori_img,(int(64),int(64)))\n",
        "    cv2.imwrite(output_image_path,new_img)\n",
        "  except:\n",
        "    print(\"image skipped\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccFpiNLQvEiM",
        "colab_type": "code",
        "outputId": "377b40fa-2632-41ab-a81c-b5614653162b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "i = 0\n",
        "src_path = '/content/carImages/'\n",
        "dst_path = '/content/carImagesResize/'\n",
        "\n",
        "if os.path.exists(dst_path):\n",
        "  shutil.rmtree(dst_path)\n",
        "shutil.rmtree(dst_path)\n",
        "if not os.path.exists(dst_path):\n",
        "    os.makedirs(dst_path)\n",
        "for filename in os.listdir(src_path):\n",
        "  filename, file_extension = os.path.splitext(filename) \n",
        "  src = src_path + filename + file_extension \n",
        "  dst = dst_path + \"img_\" + str(i) + file_extension\n",
        "  resize_images(src, dst)\n",
        "  # os.rename(src, dst) \n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image skipped\n",
            "image skipped\n",
            "image skipped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMEhotnxwKk4",
        "colab_type": "code",
        "outputId": "07c954fa-0a94-463c-ca79-6df924a79c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "print(len((os.listdir(src_path))))\n",
        "print(len((os.listdir(dst_path))))\n",
        "for filename in (os.listdir(dst_path))[:5]:\n",
        "  ori_img = cv2.imread(dst_path + filename,cv2.IMREAD_COLOR)\n",
        "  print(ori_img.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1437\n",
            "1434\n",
            "(64, 64, 3)\n",
            "(64, 64, 3)\n",
            "(64, 64, 3)\n",
            "(64, 64, 3)\n",
            "(64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGLi_5G-yIoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9BXAqk82XE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getXTrain():\n",
        "  X_train = []\n",
        "  for filename in (os.listdir(dst_path)):\n",
        "    image = cv2.imread(dst_path + filename,cv2.IMREAD_COLOR)\n",
        "    X_train.append(image)\n",
        "  return np.array(X_train) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCdlhJ5J0kkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 64\n",
        "        self.img_cols = 64\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, 1, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        # (X_train, _), (_, _) = mnist.load_data()\n",
        "        X_train = getXTrain()\n",
        "        print(X_train.shape)\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # print(\"imgs.shape = \", imgs.shape)\n",
        "            # print(imgs[0])\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new images\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            \n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                # Plot the progress\n",
        "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "                \n",
        "                # save the genImages\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/carImagesGen/%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7zNfynC0tDP",
        "colab_type": "code",
        "outputId": "a5c517b5-1c23-4fc2-b36e-3bb0ccdd0a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gen_path = \"/content/carImagesGen/\"\n",
        "if os.path.exists(gen_path):\n",
        "  shutil.rmtree(gen_path)\n",
        "if not os.path.exists(gen_path):\n",
        "    os.makedirs(gen_path)\n",
        "\n",
        "gan = GAN()\n",
        "gan.train(epochs=30000, batch_size=32, sample_interval=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_9 (Flatten)          (None, 12288)             0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 512)               6291968   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_41 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,423,553\n",
            "Trainable params: 6,423,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_43 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_44 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_45 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 12288)             12595200  \n",
            "_________________________________________________________________\n",
            "reshape_9 (Reshape)          (None, 64, 64, 1, 3)      0         \n",
            "=================================================================\n",
            "Total params: 13,285,120\n",
            "Trainable params: 13,281,536\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "(1434, 64, 64, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.690159, acc.: 39.06%] [G loss: 0.635221]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200 [D loss: 0.411243, acc.: 84.38%] [G loss: 1.908783]\n",
            "400 [D loss: 0.543537, acc.: 67.19%] [G loss: 1.416030]\n",
            "600 [D loss: 0.889799, acc.: 50.00%] [G loss: 1.531708]\n",
            "800 [D loss: 0.579543, acc.: 68.75%] [G loss: 1.510026]\n",
            "1000 [D loss: 0.582561, acc.: 70.31%] [G loss: 1.718539]\n",
            "1200 [D loss: 0.403486, acc.: 84.38%] [G loss: 2.114439]\n",
            "1400 [D loss: 0.329018, acc.: 84.38%] [G loss: 2.524767]\n",
            "1600 [D loss: 0.345726, acc.: 82.81%] [G loss: 2.653835]\n",
            "1800 [D loss: 0.253199, acc.: 92.19%] [G loss: 3.033704]\n",
            "2000 [D loss: 0.236076, acc.: 92.19%] [G loss: 2.920671]\n",
            "2200 [D loss: 0.284030, acc.: 84.38%] [G loss: 3.892900]\n",
            "2400 [D loss: 0.350718, acc.: 79.69%] [G loss: 2.879710]\n",
            "2600 [D loss: 0.197352, acc.: 92.19%] [G loss: 5.264869]\n",
            "2800 [D loss: 0.171306, acc.: 93.75%] [G loss: 3.332701]\n",
            "3000 [D loss: 0.144110, acc.: 93.75%] [G loss: 3.952491]\n",
            "3200 [D loss: 0.206475, acc.: 90.62%] [G loss: 3.866613]\n",
            "3400 [D loss: 0.233926, acc.: 92.19%] [G loss: 4.212934]\n",
            "3600 [D loss: 0.372798, acc.: 87.50%] [G loss: 4.198962]\n",
            "3800 [D loss: 0.155495, acc.: 93.75%] [G loss: 5.670484]\n",
            "4000 [D loss: 0.142505, acc.: 95.31%] [G loss: 3.896183]\n",
            "4200 [D loss: 0.138905, acc.: 93.75%] [G loss: 5.012826]\n",
            "4400 [D loss: 0.108290, acc.: 93.75%] [G loss: 5.408598]\n",
            "4600 [D loss: 0.133818, acc.: 96.88%] [G loss: 4.901165]\n",
            "4800 [D loss: 0.068630, acc.: 98.44%] [G loss: 4.258018]\n",
            "5000 [D loss: 0.338975, acc.: 85.94%] [G loss: 5.740493]\n",
            "5200 [D loss: 0.245546, acc.: 89.06%] [G loss: 4.183397]\n",
            "5400 [D loss: 0.070645, acc.: 96.88%] [G loss: 4.490077]\n",
            "5600 [D loss: 0.065992, acc.: 100.00%] [G loss: 5.357604]\n",
            "5800 [D loss: 0.151761, acc.: 98.44%] [G loss: 5.509885]\n",
            "6000 [D loss: 0.086053, acc.: 98.44%] [G loss: 5.523788]\n",
            "6200 [D loss: 0.153325, acc.: 93.75%] [G loss: 5.424494]\n",
            "6400 [D loss: 0.171197, acc.: 92.19%] [G loss: 6.316496]\n",
            "6600 [D loss: 0.291472, acc.: 90.62%] [G loss: 4.437741]\n",
            "6800 [D loss: 0.022512, acc.: 100.00%] [G loss: 6.162886]\n",
            "7000 [D loss: 0.115050, acc.: 96.88%] [G loss: 6.846974]\n",
            "7200 [D loss: 0.353829, acc.: 87.50%] [G loss: 6.114791]\n",
            "7400 [D loss: 0.108754, acc.: 96.88%] [G loss: 6.648903]\n",
            "7600 [D loss: 0.021147, acc.: 98.44%] [G loss: 8.745196]\n",
            "7800 [D loss: 0.028644, acc.: 100.00%] [G loss: 8.929632]\n",
            "8000 [D loss: 0.174077, acc.: 95.31%] [G loss: 6.752210]\n",
            "8200 [D loss: 0.046788, acc.: 100.00%] [G loss: 7.525064]\n",
            "8400 [D loss: 0.115790, acc.: 95.31%] [G loss: 5.912234]\n",
            "8600 [D loss: 0.031316, acc.: 100.00%] [G loss: 5.884377]\n",
            "8800 [D loss: 0.117988, acc.: 95.31%] [G loss: 6.167569]\n",
            "9000 [D loss: 0.056765, acc.: 98.44%] [G loss: 7.564094]\n",
            "9200 [D loss: 0.046661, acc.: 98.44%] [G loss: 6.316044]\n",
            "9400 [D loss: 0.076865, acc.: 96.88%] [G loss: 6.890574]\n",
            "9600 [D loss: 0.110612, acc.: 96.88%] [G loss: 6.370691]\n",
            "9800 [D loss: 0.258265, acc.: 90.62%] [G loss: 8.554572]\n",
            "10000 [D loss: 0.077176, acc.: 96.88%] [G loss: 6.788380]\n",
            "10200 [D loss: 0.021371, acc.: 100.00%] [G loss: 6.667130]\n",
            "10400 [D loss: 0.031375, acc.: 98.44%] [G loss: 6.636120]\n",
            "10600 [D loss: 0.018332, acc.: 100.00%] [G loss: 8.456488]\n",
            "10800 [D loss: 0.038932, acc.: 98.44%] [G loss: 6.231537]\n",
            "11000 [D loss: 0.033159, acc.: 98.44%] [G loss: 7.520681]\n",
            "11200 [D loss: 0.040239, acc.: 98.44%] [G loss: 5.961918]\n",
            "11400 [D loss: 0.032489, acc.: 98.44%] [G loss: 6.395210]\n",
            "11600 [D loss: 0.133461, acc.: 95.31%] [G loss: 7.111025]\n",
            "11800 [D loss: 0.016095, acc.: 100.00%] [G loss: 7.172307]\n",
            "12000 [D loss: 0.029237, acc.: 100.00%] [G loss: 7.649836]\n",
            "12200 [D loss: 0.087086, acc.: 98.44%] [G loss: 5.441149]\n",
            "12400 [D loss: 0.063627, acc.: 95.31%] [G loss: 9.189860]\n",
            "12600 [D loss: 0.036637, acc.: 96.88%] [G loss: 6.501451]\n",
            "12800 [D loss: 0.150989, acc.: 96.88%] [G loss: 5.562204]\n",
            "13000 [D loss: 0.063870, acc.: 96.88%] [G loss: 7.285580]\n",
            "13200 [D loss: 0.143883, acc.: 96.88%] [G loss: 7.394899]\n",
            "13400 [D loss: 0.014235, acc.: 100.00%] [G loss: 8.260025]\n",
            "13600 [D loss: 0.007819, acc.: 100.00%] [G loss: 7.931499]\n",
            "13800 [D loss: 0.016648, acc.: 100.00%] [G loss: 6.856703]\n",
            "14000 [D loss: 0.056874, acc.: 98.44%] [G loss: 8.299501]\n",
            "14200 [D loss: 0.264824, acc.: 96.88%] [G loss: 7.588308]\n",
            "14400 [D loss: 0.093532, acc.: 96.88%] [G loss: 8.055132]\n",
            "14600 [D loss: 0.009212, acc.: 100.00%] [G loss: 7.103129]\n",
            "14800 [D loss: 0.012458, acc.: 100.00%] [G loss: 6.580618]\n",
            "15000 [D loss: 0.106904, acc.: 96.88%] [G loss: 7.203588]\n",
            "15200 [D loss: 0.079990, acc.: 95.31%] [G loss: 7.287839]\n",
            "15400 [D loss: 0.076414, acc.: 98.44%] [G loss: 6.754033]\n",
            "15600 [D loss: 0.015574, acc.: 100.00%] [G loss: 9.264237]\n",
            "15800 [D loss: 0.174120, acc.: 95.31%] [G loss: 10.743609]\n",
            "16000 [D loss: 0.012102, acc.: 100.00%] [G loss: 6.271491]\n",
            "16200 [D loss: 0.091443, acc.: 96.88%] [G loss: 7.469285]\n",
            "16400 [D loss: 0.120338, acc.: 96.88%] [G loss: 9.275669]\n",
            "16600 [D loss: 0.032302, acc.: 98.44%] [G loss: 8.107038]\n",
            "16800 [D loss: 0.006279, acc.: 100.00%] [G loss: 7.620200]\n",
            "17000 [D loss: 0.006974, acc.: 100.00%] [G loss: 9.335871]\n",
            "17200 [D loss: 0.086187, acc.: 98.44%] [G loss: 8.095322]\n",
            "17400 [D loss: 0.009473, acc.: 100.00%] [G loss: 8.385159]\n",
            "17600 [D loss: 0.007106, acc.: 100.00%] [G loss: 8.889548]\n",
            "17800 [D loss: 0.139127, acc.: 98.44%] [G loss: 8.297908]\n",
            "18000 [D loss: 0.045901, acc.: 96.88%] [G loss: 9.749784]\n",
            "18200 [D loss: 0.032509, acc.: 98.44%] [G loss: 7.445957]\n",
            "18400 [D loss: 0.071904, acc.: 96.88%] [G loss: 8.144188]\n",
            "18600 [D loss: 0.018278, acc.: 100.00%] [G loss: 9.074581]\n",
            "18800 [D loss: 0.003767, acc.: 100.00%] [G loss: 7.846071]\n",
            "19000 [D loss: 0.013348, acc.: 100.00%] [G loss: 7.012067]\n",
            "19200 [D loss: 0.099567, acc.: 96.88%] [G loss: 9.663687]\n",
            "19400 [D loss: 0.005605, acc.: 100.00%] [G loss: 11.433876]\n",
            "19600 [D loss: 0.014988, acc.: 100.00%] [G loss: 9.827991]\n",
            "19800 [D loss: 0.472881, acc.: 95.31%] [G loss: 13.661016]\n",
            "20000 [D loss: 0.331848, acc.: 95.31%] [G loss: 7.119504]\n",
            "20200 [D loss: 0.121534, acc.: 95.31%] [G loss: 8.600156]\n",
            "20400 [D loss: 0.005656, acc.: 100.00%] [G loss: 11.037405]\n",
            "20600 [D loss: 0.007413, acc.: 100.00%] [G loss: 10.554700]\n",
            "20800 [D loss: 0.102736, acc.: 95.31%] [G loss: 9.082066]\n",
            "21000 [D loss: 0.104290, acc.: 95.31%] [G loss: 9.559846]\n",
            "21200 [D loss: 0.028444, acc.: 98.44%] [G loss: 8.560490]\n",
            "21400 [D loss: 0.021569, acc.: 100.00%] [G loss: 8.688660]\n",
            "21600 [D loss: 0.002373, acc.: 100.00%] [G loss: 8.549713]\n",
            "21800 [D loss: 0.072839, acc.: 96.88%] [G loss: 11.295752]\n",
            "22000 [D loss: 0.005486, acc.: 100.00%] [G loss: 10.557518]\n",
            "22200 [D loss: 0.004969, acc.: 100.00%] [G loss: 8.145897]\n",
            "22400 [D loss: 0.226025, acc.: 93.75%] [G loss: 9.737319]\n",
            "22600 [D loss: 0.023659, acc.: 100.00%] [G loss: 8.103544]\n",
            "22800 [D loss: 0.003704, acc.: 100.00%] [G loss: 7.606651]\n",
            "23000 [D loss: 0.003161, acc.: 100.00%] [G loss: 9.319502]\n",
            "23200 [D loss: 0.012407, acc.: 100.00%] [G loss: 9.462355]\n",
            "23400 [D loss: 0.191185, acc.: 96.88%] [G loss: 9.263958]\n",
            "23600 [D loss: 0.165809, acc.: 95.31%] [G loss: 10.115984]\n",
            "23800 [D loss: 0.001543, acc.: 100.00%] [G loss: 10.230541]\n",
            "24000 [D loss: 0.006667, acc.: 100.00%] [G loss: 7.387915]\n",
            "24200 [D loss: 0.001882, acc.: 100.00%] [G loss: 9.358694]\n",
            "24400 [D loss: 0.010156, acc.: 100.00%] [G loss: 7.282323]\n",
            "24600 [D loss: 0.031844, acc.: 98.44%] [G loss: 9.855200]\n",
            "24800 [D loss: 0.015442, acc.: 98.44%] [G loss: 8.872984]\n",
            "25000 [D loss: 0.070884, acc.: 96.88%] [G loss: 12.594807]\n",
            "25200 [D loss: 0.351097, acc.: 96.88%] [G loss: 9.786006]\n",
            "25400 [D loss: 0.003235, acc.: 100.00%] [G loss: 9.969658]\n",
            "25600 [D loss: 0.061414, acc.: 96.88%] [G loss: 9.536077]\n",
            "25800 [D loss: 0.006501, acc.: 100.00%] [G loss: 11.886988]\n",
            "26000 [D loss: 0.021412, acc.: 100.00%] [G loss: 7.425143]\n",
            "26200 [D loss: 0.013339, acc.: 98.44%] [G loss: 10.860851]\n",
            "26400 [D loss: 0.005511, acc.: 100.00%] [G loss: 10.375048]\n",
            "26600 [D loss: 0.022557, acc.: 98.44%] [G loss: 8.036161]\n",
            "26800 [D loss: 0.000154, acc.: 100.00%] [G loss: 12.303837]\n",
            "27000 [D loss: 0.011541, acc.: 100.00%] [G loss: 12.393088]\n",
            "27200 [D loss: 0.021733, acc.: 98.44%] [G loss: 10.162344]\n",
            "27400 [D loss: 0.006131, acc.: 100.00%] [G loss: 8.118083]\n",
            "27600 [D loss: 0.004040, acc.: 100.00%] [G loss: 9.185386]\n",
            "27800 [D loss: 0.001695, acc.: 100.00%] [G loss: 9.898864]\n",
            "28000 [D loss: 0.159229, acc.: 93.75%] [G loss: 12.427179]\n",
            "28200 [D loss: 0.014219, acc.: 98.44%] [G loss: 9.843763]\n",
            "28400 [D loss: 0.000404, acc.: 100.00%] [G loss: 9.531273]\n",
            "28600 [D loss: 0.000357, acc.: 100.00%] [G loss: 10.888556]\n",
            "28800 [D loss: 0.176606, acc.: 96.88%] [G loss: 10.617847]\n",
            "29000 [D loss: 0.089766, acc.: 96.88%] [G loss: 7.595840]\n",
            "29200 [D loss: 0.005692, acc.: 100.00%] [G loss: 9.998154]\n",
            "29400 [D loss: 0.003889, acc.: 100.00%] [G loss: 7.874373]\n",
            "29600 [D loss: 0.035875, acc.: 96.88%] [G loss: 12.134463]\n",
            "29800 [D loss: 0.000746, acc.: 100.00%] [G loss: 13.034641]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXDGDwyS4E-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}