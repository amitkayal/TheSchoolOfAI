{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session4_DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "aNyZv-Ec52ot",
        "zByEi95J86RD",
        "SThHFUgs3mRF",
        "-a5NgpGH3tK1",
        "OL6bkTDh3ucc",
        "FdGYj_N44F3n",
        "35xBkA2m4Lef",
        "0xAGByuJ4Qs8",
        "BwErANNj7d54",
        "_AJLttwY7gne",
        "NaCBYz6olAt-",
        "0cUeG8m3z94m",
        "kP7Nppbu048P",
        "honftgVA08Lm",
        "7Q0Wme9apxUa",
        "PODiuFsA0NAQ",
        "NJTpGlgkyj1h",
        "Dros2PrD0-jO",
        "hTseAGfI1CsU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMXd3mEM2pcv",
        "colab_type": "text"
      },
      "source": [
        "#Session 4 - Architectural Basics\n",
        "##After 4 code iterations achieve:\n",
        "###1. 99.4% accuracy\n",
        "###2. Less than 15k Parameters\n",
        "###3. Have started from a Vanilla network (no BN, DropOut, LR, larger batch size,change in Optimizer, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "### **Import Libraries and modules**\n",
        "1. import the basic libraries\n",
        "2. import libraries required for building up the model using keras\n",
        "3. import the mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Activation\n",
        "\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets\n",
        "Load data from mnist dataset and split the data between training dataset and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SThHFUgs3mRF",
        "colab_type": "text"
      },
      "source": [
        "###**Print and Plot**:\n",
        "1. Print the shape for the trainig dataset\n",
        "2. Import pyplot for plotting the input image\n",
        "3. Plot the Input Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "e96dc37f-9c83-4987-be94-5193c7049bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0763e44630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a5NgpGH3tK1",
        "colab_type": "text"
      },
      "source": [
        "###**Reshape**:\n",
        "1. Reshape the training dateset to 28*28 size with 1 channel.\n",
        "2. Reshape the testing dateset to 28*28 size with 1 channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6bkTDh3ucc",
        "colab_type": "text"
      },
      "source": [
        "###**Type Conversion and Scalling:**\n",
        "1. Convert the input data type to float32.\n",
        "2. Normalise the input data within the range of 0 and 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdGYj_N44F3n",
        "colab_type": "text"
      },
      "source": [
        "###**Print Y Train:**\n",
        "1. Print the y train first 10 values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "0812365f-c0b0-481b-ee5c-464d6bca8860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35xBkA2m4Lef",
        "colab_type": "text"
      },
      "source": [
        "###**One Hot Encoding of Labels:**\n",
        "1. Convert the 1D class arrays of output data into 10 dimesional class matrices - basically one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xAGByuJ4Qs8",
        "colab_type": "text"
      },
      "source": [
        "###**Print Y Train after encoding:**\n",
        "1. Print the y train first 10 values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "1b86c831-02cf-4843-e471-6f3914092322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6jX2i4U4fVJ",
        "colab_type": "text"
      },
      "source": [
        "#**Model Creation Using Keras**\n",
        "The following code defines the architecture/model of our neural network in which we will add layers in the network using keras:\n",
        "1.   **Initializing a sequential model**: Stack layers on top of one another, it also ensures that the input and output sizes of successive layers are compatible. \n",
        "2.   **Adding Convolution Layers**: While adding the first convolution layer we need to specify the input size as well. In the next convolution layers we do not need an input size since they will receive the input size from the preceeding layers.\n",
        "3. **Adding Max Pooling Layers**: We add them to resuce the dimensions of image extracting the maximum out of the (2,2) pool and pass them forward.\n",
        "4. **Adding Flatten Layer**: When finally we reach the output size of 1X1X10, we flatten the output channel-wise into output of shape 10 (1-dimension).\n",
        "5. **Adding Softmax Layer**: This layer converts the last layer outuput into probabolity distribution of 10 classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwErANNj7d54",
        "colab_type": "text"
      },
      "source": [
        "##Vanilla DNN\n",
        "\n",
        "**1. Total params: 13,496**\n",
        "\n",
        "**2. Trainable params: 13,496**\n",
        "\n",
        "**3. Non-trainable params: 0**\n",
        "\n",
        "##We are getting the validation accuracy = 99.08%\n",
        "#### On Epoch 10 the following results:\n",
        "loss: 0.0195 || acc: 0.9932 || val_loss: 0.0286 || **val_acc: 0.9908** ||\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0SbSqE-oYqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "89f608f6-2719-4737-d3ad-b451d7ffdeb6"
      },
      "source": [
        "########################################\n",
        "model = Sequential()\n",
        "########################################\n",
        "\n",
        "# All shapes are in the form [height, width, num_channels]\n",
        "\n",
        "# INPUT SIZE : [28, 28, 1] | OUTPUT SIZE : [26, 26, 8] | GRF_of_input : [1, 1]\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "\n",
        "# INPUT SIZE : [26, 26, 8] | OUTPUT SIZE : [24, 24, 16] | GRF_of_input : [3, 3]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "\n",
        "# INPUT SIZE : [24, 24, 16] | OUTPUT SIZE : [22, 22, 24] | GRF_of_input : [5, 5]\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [22, 22, 24] | OUTPUT SIZE : [11, 11, 24] | GRF_of_input : [7, 7]\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [11, 11, 24] | OUTPUT SIZE : [11, 11, 8] | GRF_of_input : [14, 14]\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "\n",
        "# INPUT SIZE : [11, 11, 8] | OUTPUT SIZE : [9, 9, 16] | GRF_of_input : [14, 14]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "\n",
        "# INPUT SIZE : [9, 9, 16] | OUTPUT SIZE : [7, 7, 16] | GRF_of_input : [16, 16]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [7, 7, 16] | OUTPUT SIZE : [7, 7, 10] | GRF_of_input : [18, 18]\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))\n",
        "\n",
        "# INPUT SIZE : [7, 7, 10] | OUTPUT SIZE : [1, 1, 10] | GRF_of_input : [18, 18]\n",
        "model.add(Convolution2D(10, 7, 7))\n",
        "\n",
        "########################################\n",
        "# INPUT SIZE : [1, 1, 10] | OUTPUT SIZE : [10] | GRF_of_input : [31, 31]\n",
        "model.add(Flatten())\n",
        "\n",
        "# INPUT SIZE : [10] | OUTPUT SIZE : [10] | GRF_of_input : [31, 31]\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "########################################\n",
        "# Printing out the model summary\n",
        "model.summary()\n",
        "\n",
        "# saving the model to reload model with initial weights while trying out different optimizations\n",
        "model.save_weights('model_vanilla.h5')\n",
        "########################################"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 22, 22, 24)        3480      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 7, 7, 10)          170       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,496\n",
            "Trainable params: 13,496\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXvFgY8m_krr",
        "colab_type": "text"
      },
      "source": [
        "###**Compiling the Model**\n",
        "\n",
        "Compiling means that the keras will create the computational graph using tensorflow backend. \n",
        "\n",
        "For this to happen, we need to provide:\n",
        "1. **Loss Function:** It is a mathematical function that calculates some sort of differentiable distance measure between the network's output and the actual output for each class. During training, we average out these losses and backpropagate them through the network. In this case - we use categorical cross-entropy.\n",
        "2. **Optimizer:** Optimizer algorithm manipulates the learning rate per time-step so as to update the weights in a manner that does not lead to a swinging behaviour around the minimum. A few examples are AdaGrad, Momentum etc. but Adam is what is mostly used by everyone.\n",
        "3. **Accuracy Metric:** This is a metric that measures the performance of the network. In the case of classification this is accuracy (i.e. number_of_correct_predictions / number_of_total_examples_in_batch).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Training the Model**\n",
        "\n",
        "We use model.fit for training the model. We pass the training dataset, batch size and number of epochs as parameter to the function.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6BMWovKhJEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51f8d9a9-26d5-43bb-88a0-9b3ed14358c0"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.load_weights('model_vanilla.h5')\n",
        "model.fit(X_train, Y_train, batch_size=32, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.2280 - acc: 0.9295 - val_loss: 0.0771 - val_acc: 0.9765\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0759 - acc: 0.9773 - val_loss: 0.0536 - val_acc: 0.9832\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0539 - acc: 0.9836 - val_loss: 0.0422 - val_acc: 0.9859\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0446 - acc: 0.9864 - val_loss: 0.0373 - val_acc: 0.9882\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0365 - acc: 0.9888 - val_loss: 0.0351 - val_acc: 0.9875\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0318 - acc: 0.9900 - val_loss: 0.0374 - val_acc: 0.9875\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 23s 389us/step - loss: 0.0285 - acc: 0.9912 - val_loss: 0.0315 - val_acc: 0.9903\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0252 - acc: 0.9920 - val_loss: 0.0336 - val_acc: 0.9888\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0218 - acc: 0.9932 - val_loss: 0.0387 - val_acc: 0.9874\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 23s 379us/step - loss: 0.0195 - acc: 0.9932 - val_loss: 0.0286 - val_acc: 0.9908\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0381 - val_acc: 0.9876\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0344 - val_acc: 0.9897\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.0323 - val_acc: 0.9899\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0367 - val_acc: 0.9886\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 22s 375us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0470 - val_acc: 0.9880\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0424 - val_acc: 0.9893\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0401 - val_acc: 0.9905\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0403 - val_acc: 0.9897\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0457 - val_acc: 0.9896\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0422 - val_acc: 0.9894\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0419 - val_acc: 0.9908\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0517 - val_acc: 0.9876\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0453 - val_acc: 0.9889\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0616 - val_acc: 0.9870\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0505 - val_acc: 0.9888\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0482 - val_acc: 0.9886\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0542 - val_acc: 0.9877\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0448 - val_acc: 0.9899\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0490 - val_acc: 0.9898\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0495 - val_acc: 0.9894\n",
            "[0.04948874166003443, 0.9894]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AJLttwY7gne",
        "colab_type": "text"
      },
      "source": [
        "## DNN - Adding Batch Normalization and Dropout\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Batch Normalization\n",
        "> It is used to normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. \n",
        "\n",
        "> Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the **internal covariate shift** problem.\n",
        "Batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.\n",
        "\n",
        "> **Internal covariate shift** -  Each layer must learn to adapt themselves to a new distribution in every training step which slows down the training process.\n",
        "\n",
        "**Steps for BN:**\n",
        "1. Calculate the mean and variance of the layers input.\n",
        "2. Normalize the layer inputs using the previously calculated batch statistics.\n",
        "3. Scale and shift in order to obtain the output of the layer.\n",
        "**P.S - γ and β are learned during training along with the original parameters of the network.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Dropout\n",
        "> The term “Dropout” refers to dropping out units (both hidden and visible) in a neural network. Dropout ignores units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, I mean these units are not considered during a particular forward or backward pass.\n",
        "\n",
        "> Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.During training, some number of layer outputs are randomly ignored or “dropped out.”\n",
        "####Why do we need Dropout?\n",
        "\n",
        "> To prevent over-fitting\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Validation Accuracy = 99.42%\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSBIYZO97sY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "969068ab-eb25-415c-d13e-7c68c17ea608"
      },
      "source": [
        "########################################\n",
        "model = Sequential()\n",
        "########################################\n",
        "\n",
        "# All shapes are in the form [height, width, num_channels]\n",
        "\n",
        "# INPUT SIZE : [28, 28, 1] | OUTPUT SIZE : [26, 26, 8] | GRF_of_input : [1, 1]\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# INPUT SIZE : [26, 26, 8] | OUTPUT SIZE : [24, 24, 16] | GRF_of_input : [3, 3]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# INPUT SIZE : [24, 24, 16] | OUTPUT SIZE : [22, 22, 24] | GRF_of_input : [5, 5]\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "########################################\n",
        "# 0.15% of the input units are set to 0 randomly\n",
        "model.add(Dropout(0.15))\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [22, 22, 24] | OUTPUT SIZE : [11, 11, 24] | GRF_of_input : [7, 7]\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [11, 11, 24] | OUTPUT SIZE : [11, 11, 8] | GRF_of_input : [14, 14]\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# INPUT SIZE : [11, 11, 8] | OUTPUT SIZE : [9, 9, 16] | GRF_of_input : [14, 14]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# INPUT SIZE : [9, 9, 16] | OUTPUT SIZE : [7, 7, 16] | GRF_of_input : [16, 16]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "########################################\n",
        "# 0.15% of the input units are set to 0 randomly\n",
        "model.add(Dropout(0.15))\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [7, 7, 16] | OUTPUT SIZE : [7, 7, 10] | GRF_of_input : [18, 18]\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# INPUT SIZE : [7, 7, 10] | OUTPUT SIZE : [1, 1, 10] | GRF_of_input : [18, 18]\n",
        "model.add(Convolution2D(10, 7, 7))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "########################################\n",
        "# INPUT SIZE : [1, 1, 10] | OUTPUT SIZE : [10] | GRF_of_input : [31, 31]\n",
        "model.add(Flatten())\n",
        "\n",
        "# INPUT SIZE : [10] | OUTPUT SIZE : [10] | GRF_of_input : [31, 31]\n",
        "model.add(Activation('softmax'))\n",
        "########################################\n",
        "# Printing out the model summary\n",
        "model.summary()\n",
        "\n",
        "# saving the model to reload model with initial weights while trying out different optimizations\n",
        "model.save_weights('model_BN_AND_DROPOUT.h5')\n",
        "########################################"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "W0816 09:52:50.929826 139672295782272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "W0816 09:52:51.238708 139672295782272 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 24)        3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 24)        96        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 22, 22, 24)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 24)        96        \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 11, 11, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 7, 7, 10)          170       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 1, 1, 10)          4910      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,024\n",
            "Trainable params: 13,760\n",
            "Non-trainable params: 264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaCBYz6olAt-",
        "colab_type": "text"
      },
      "source": [
        "### Validation Accuracy = 99.42%\n",
        "1. Total params: 14,024\n",
        "2. Trainable params: 13,760\n",
        "3. Non-trainable params: 264\n",
        "\n",
        "### Epoch 17 - loss: 0.0291 || acc: 0.9916 || val_loss: 0.0233 || val_acc: 0.9942\n",
        "\n",
        "---\n",
        "\n",
        " ## Comparison With Vanilla DNN\n",
        " 1. With Vanilla DNN, we were getting **99.08% validation accuracy** and after adding Batch Normalisation and Dropout we get **99.42% validation accuracy**.\n",
        " 2. We used Batch Normalisation after every single layer to normalise the input image.\n",
        " 3. We used 2 Dropout Layers with **0.15 rate** in our network for randomly setting input units to 0 at each update during training time, which helps prevent overfitting.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQcUG2JQzlxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9930ead-62f9-44d6-a15c-e333953d63ea"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=32, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 43s 710us/step - loss: 0.4145 - acc: 0.9235 - val_loss: 0.0990 - val_acc: 0.9841\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 41s 678us/step - loss: 0.1557 - acc: 0.9684 - val_loss: 0.0577 - val_acc: 0.9883\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 43s 711us/step - loss: 0.1061 - acc: 0.9763 - val_loss: 0.0464 - val_acc: 0.9899\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 42s 705us/step - loss: 0.0836 - acc: 0.9807 - val_loss: 0.0436 - val_acc: 0.9897\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 42s 693us/step - loss: 0.0696 - acc: 0.9835 - val_loss: 0.0312 - val_acc: 0.9927\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 42s 702us/step - loss: 0.0626 - acc: 0.9845 - val_loss: 0.0373 - val_acc: 0.9897\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0549 - acc: 0.9859 - val_loss: 0.0300 - val_acc: 0.9922\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 42s 693us/step - loss: 0.0491 - acc: 0.9873 - val_loss: 0.0334 - val_acc: 0.9914\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 42s 695us/step - loss: 0.0447 - acc: 0.9887 - val_loss: 0.0279 - val_acc: 0.9920\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 41s 687us/step - loss: 0.0417 - acc: 0.9889 - val_loss: 0.0325 - val_acc: 0.9909\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0396 - acc: 0.9893 - val_loss: 0.0231 - val_acc: 0.9936\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.0364 - acc: 0.9903 - val_loss: 0.0295 - val_acc: 0.9908\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 42s 693us/step - loss: 0.0342 - acc: 0.9905 - val_loss: 0.0261 - val_acc: 0.9918\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 42s 695us/step - loss: 0.0345 - acc: 0.9900 - val_loss: 0.0282 - val_acc: 0.9915\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 41s 688us/step - loss: 0.0313 - acc: 0.9913 - val_loss: 0.0238 - val_acc: 0.9923\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.0259 - val_acc: 0.9923\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 41s 682us/step - loss: 0.0291 - acc: 0.9916 - val_loss: 0.0233 - val_acc: 0.9942\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 42s 693us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 0.0256 - val_acc: 0.9923\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 42s 706us/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 41s 678us/step - loss: 0.0252 - acc: 0.9931 - val_loss: 0.0270 - val_acc: 0.9914\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 41s 688us/step - loss: 0.0251 - acc: 0.9929 - val_loss: 0.0239 - val_acc: 0.9935\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 40s 675us/step - loss: 0.0240 - acc: 0.9933 - val_loss: 0.0238 - val_acc: 0.9925\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 41s 686us/step - loss: 0.0250 - acc: 0.9930 - val_loss: 0.0247 - val_acc: 0.9923\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 41s 682us/step - loss: 0.0225 - acc: 0.9937 - val_loss: 0.0235 - val_acc: 0.9921\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 41s 678us/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9932\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0221 - val_acc: 0.9929\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 41s 677us/step - loss: 0.0216 - acc: 0.9939 - val_loss: 0.0211 - val_acc: 0.9929\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 40s 675us/step - loss: 0.0208 - acc: 0.9939 - val_loss: 0.0239 - val_acc: 0.9917\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 41s 684us/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0251 - val_acc: 0.9916\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 41s 684us/step - loss: 0.0204 - acc: 0.9941 - val_loss: 0.0228 - val_acc: 0.9929\n",
            "[0.022777376713452396, 0.9929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cUeG8m3z94m",
        "colab_type": "text"
      },
      "source": [
        "## Improving the Batch Size\n",
        "###Batch Size\n",
        "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. A training dataset can be divided into one or more batches. \n",
        "> There are 3 types of samples distribution based on different batch size:\n",
        "* **Batch Gradient Descent**: Batch Size = Size of Training Set\n",
        "* **Stochastic Gradient Descent**: Batch Size = 1\n",
        "* **Mini-Batch Gradient Descent**: == 1 < Batch Size < Size of Training Set\n",
        "\n",
        "> **Small Batch Size** - They are noisy, offering a regularizing effect and lower generalization error. It is easier to fit one batch worth of training data in memory (i.e. when using a GPU).\n",
        "\n",
        "> **Large Batch Size** - To allow larger learning rate and faster convergence and there are less number of batches so the training is faster.\n",
        "It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize.\n",
        "\n",
        "---\n",
        "\n",
        "In the following 3 cells we have tried out our latest DNN with **diffent batch sizes**:\n",
        "###1. Batch Size = 50, Validation Accuracy = 99.39%\n",
        "**Epoch 17/30 -**\n",
        "|| loss: 0.0268 || acc: 0.9923 || val_loss: 0.0199 || **val_acc: 0.9939**\n",
        "\n",
        "###2. Batch Size = 75, Validation Accuracy = 99.38%\n",
        "**Epoch 12/30 -**\n",
        "|| loss: 0.0314 || acc: 0.9917 || val_loss: 0.0223 || **val_acc: 0.9938**\n",
        "\n",
        "###3. Batch Size = 100, Validation Accuracy = 99.41%\n",
        "**Epoch 26/30 -**\n",
        "|| loss: 0.0160 || acc: 0.9954 || val_loss: 0.0194 || **val_acc: 0.9941**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP7Nppbu048P",
        "colab_type": "text"
      },
      "source": [
        "### Batch Size = 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdh6adYr0B6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c85bc5ae-ef3a-4ea1-f68a-7b782c5a4805"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=50, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.4474 - acc: 0.9211 - val_loss: 0.1484 - val_acc: 0.9799\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 28s 474us/step - loss: 0.1531 - acc: 0.9738 - val_loss: 0.0713 - val_acc: 0.9874\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 28s 471us/step - loss: 0.1011 - acc: 0.9801 - val_loss: 0.0604 - val_acc: 0.9857\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0779 - acc: 0.9829 - val_loss: 0.0448 - val_acc: 0.9902\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 29s 480us/step - loss: 0.0649 - acc: 0.9847 - val_loss: 0.0408 - val_acc: 0.9885\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 0.0567 - acc: 0.9857 - val_loss: 0.0392 - val_acc: 0.9890\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 29s 489us/step - loss: 0.0517 - acc: 0.9869 - val_loss: 0.0411 - val_acc: 0.9893\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 29s 488us/step - loss: 0.0457 - acc: 0.9883 - val_loss: 0.0273 - val_acc: 0.9930\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0438 - acc: 0.9886 - val_loss: 0.0280 - val_acc: 0.9918\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 30s 507us/step - loss: 0.0379 - acc: 0.9898 - val_loss: 0.0250 - val_acc: 0.9925\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 0.0379 - acc: 0.9898 - val_loss: 0.0277 - val_acc: 0.9914\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 31s 518us/step - loss: 0.0336 - acc: 0.9911 - val_loss: 0.0247 - val_acc: 0.9927\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0330 - acc: 0.9911 - val_loss: 0.0236 - val_acc: 0.9929\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 30s 498us/step - loss: 0.0311 - acc: 0.9914 - val_loss: 0.0230 - val_acc: 0.9935\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 30s 500us/step - loss: 0.0292 - acc: 0.9917 - val_loss: 0.0245 - val_acc: 0.9924\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0270 - acc: 0.9926 - val_loss: 0.0272 - val_acc: 0.9919\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 31s 517us/step - loss: 0.0268 - acc: 0.9923 - val_loss: 0.0199 - val_acc: 0.9939\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0251 - acc: 0.9932 - val_loss: 0.0218 - val_acc: 0.9934\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 0.0252 - acc: 0.9928 - val_loss: 0.0231 - val_acc: 0.9927\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 30s 499us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0214 - val_acc: 0.9939\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 29s 490us/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 30s 507us/step - loss: 0.0211 - acc: 0.9942 - val_loss: 0.0232 - val_acc: 0.9933\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 31s 520us/step - loss: 0.0218 - acc: 0.9937 - val_loss: 0.0214 - val_acc: 0.9933\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 30s 506us/step - loss: 0.0200 - acc: 0.9944 - val_loss: 0.0227 - val_acc: 0.9935\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 30s 500us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0226 - val_acc: 0.9933\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.0196 - acc: 0.9946 - val_loss: 0.0236 - val_acc: 0.9929\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0242 - val_acc: 0.9929\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0209 - val_acc: 0.9934\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 29s 484us/step - loss: 0.0164 - acc: 0.9955 - val_loss: 0.0227 - val_acc: 0.9930\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0221 - val_acc: 0.9929\n",
            "[0.02211758792460896, 0.9929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honftgVA08Lm",
        "colab_type": "text"
      },
      "source": [
        "### Batch Size = 75"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPYcAZ8z0s2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "502dba0d-412c-4533-babc-ab5aa2a0f3b9"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=75, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.4891 - acc: 0.9177 - val_loss: 0.1847 - val_acc: 0.9813\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.1646 - acc: 0.9783 - val_loss: 0.0867 - val_acc: 0.9877\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.1038 - acc: 0.9839 - val_loss: 0.0781 - val_acc: 0.9852\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0782 - acc: 0.9853 - val_loss: 0.0520 - val_acc: 0.9901\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0629 - acc: 0.9876 - val_loss: 0.0435 - val_acc: 0.9894\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.0551 - acc: 0.9877 - val_loss: 0.0346 - val_acc: 0.9917\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.0486 - acc: 0.9888 - val_loss: 0.0305 - val_acc: 0.9913\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0426 - acc: 0.9901 - val_loss: 0.0283 - val_acc: 0.9928\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.0371 - acc: 0.9913 - val_loss: 0.0311 - val_acc: 0.9919\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0338 - acc: 0.9915 - val_loss: 0.0248 - val_acc: 0.9928\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0344 - acc: 0.9911 - val_loss: 0.0270 - val_acc: 0.9923\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0314 - acc: 0.9917 - val_loss: 0.0223 - val_acc: 0.9938\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0284 - acc: 0.9923 - val_loss: 0.0263 - val_acc: 0.9923\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0281 - acc: 0.9917 - val_loss: 0.0231 - val_acc: 0.9924\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0267 - acc: 0.9925 - val_loss: 0.0231 - val_acc: 0.9923\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0254 - acc: 0.9932 - val_loss: 0.0259 - val_acc: 0.9920\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.0239 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9929\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0233 - acc: 0.9937 - val_loss: 0.0378 - val_acc: 0.9896\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0214 - acc: 0.9937 - val_loss: 0.0199 - val_acc: 0.9936\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0206 - acc: 0.9942 - val_loss: 0.0228 - val_acc: 0.9922\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0208 - acc: 0.9941 - val_loss: 0.0229 - val_acc: 0.9931\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0191 - acc: 0.9948 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0196 - acc: 0.9943 - val_loss: 0.0220 - val_acc: 0.9937\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.0178 - acc: 0.9948 - val_loss: 0.0255 - val_acc: 0.9925\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0184 - acc: 0.9946 - val_loss: 0.0248 - val_acc: 0.9923\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.0176 - acc: 0.9950 - val_loss: 0.0218 - val_acc: 0.9927\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0244 - val_acc: 0.9933\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0220 - val_acc: 0.9934\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0170 - acc: 0.9950 - val_loss: 0.0220 - val_acc: 0.9937\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.0157 - acc: 0.9952 - val_loss: 0.0272 - val_acc: 0.9921\n",
            "[0.02717758358961437, 0.9921]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q0Wme9apxUa",
        "colab_type": "text"
      },
      "source": [
        "### Batch Size = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvCgFCavpo_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04db6d0a-cd47-4e9d-e1b5-b245c338acbd"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=100, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.5167 - acc: 0.9165 - val_loss: 0.2181 - val_acc: 0.9803\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.1869 - acc: 0.9781 - val_loss: 0.1063 - val_acc: 0.9874\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.1187 - acc: 0.9834 - val_loss: 0.0720 - val_acc: 0.9894\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0885 - acc: 0.9859 - val_loss: 0.0533 - val_acc: 0.9907\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0694 - acc: 0.9875 - val_loss: 0.0504 - val_acc: 0.9908\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0594 - acc: 0.9886 - val_loss: 0.0401 - val_acc: 0.9911\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0516 - acc: 0.9889 - val_loss: 0.0374 - val_acc: 0.9906\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0458 - acc: 0.9900 - val_loss: 0.0349 - val_acc: 0.9911\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0404 - acc: 0.9907 - val_loss: 0.0276 - val_acc: 0.9922\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0374 - acc: 0.9913 - val_loss: 0.0302 - val_acc: 0.9922\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0345 - acc: 0.9915 - val_loss: 0.0263 - val_acc: 0.9931\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0318 - acc: 0.9918 - val_loss: 0.0276 - val_acc: 0.9917\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0298 - acc: 0.9929 - val_loss: 0.0279 - val_acc: 0.9913\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0273 - acc: 0.9930 - val_loss: 0.0242 - val_acc: 0.9932\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0271 - acc: 0.9928 - val_loss: 0.0328 - val_acc: 0.9902\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 16s 270us/step - loss: 0.0251 - acc: 0.9930 - val_loss: 0.0268 - val_acc: 0.9921\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0255 - acc: 0.9931 - val_loss: 0.0209 - val_acc: 0.9938\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0223 - acc: 0.9938 - val_loss: 0.0267 - val_acc: 0.9929\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0217 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9937\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0206 - acc: 0.9943 - val_loss: 0.0217 - val_acc: 0.9934\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0210 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9922\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0204 - acc: 0.9944 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.0214 - val_acc: 0.9931\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0247 - val_acc: 0.9922\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0219 - val_acc: 0.9936\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0160 - acc: 0.9954 - val_loss: 0.0194 - val_acc: 0.9941\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.0214 - val_acc: 0.9929\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0260 - val_acc: 0.9923\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0223 - val_acc: 0.9932\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.0242 - val_acc: 0.9929\n",
            "[0.024238967639626935, 0.9929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PODiuFsA0NAQ",
        "colab_type": "text"
      },
      "source": [
        "##Improving the Learning Rate\n",
        "\n",
        "###Learning Rate\n",
        "The amount of change to the model during each step of this search process, or the step size, is called the “learning rate”. \n",
        "\n",
        "Learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
        "\n",
        "During training, the backpropagation of error estimates the amount of error for which the weights of a node in the network are responsible. Instead of updating the weight with the full amount, it is scaled by the learning rate.\n",
        "\n",
        "Learning rate controls how quickly or slowly a neural network model learns a problem.\n",
        "\n",
        "**Large learning rate** allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights.\n",
        "\n",
        "**Small learning rate** may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In the following cells we have tried out our latest DNN with **Learning Rate**:\n",
        "###1. Learning Rate = 0.01, Optimizer = Adam, Batch Size = 100, Validation Accuracy = 99.38%\n",
        "**Epoch 27/30 -**\n",
        "|| loss: 0.0146 || acc: 0.9951 || val_loss: 0.0212 || **val_acc: 0.9938**\n",
        "\n",
        "###2. Learning Rate = 0.005, Optimizer = Adam, Batch Size = 100, Validation Accuracy = 99.41%\n",
        "**Epoch 30/30 -**\n",
        "|| loss: 0.0124 || acc: 0.9959 || val_loss: 0.0224 || **val_acc: 0.9941**\n",
        "\n",
        "###3. Learning Rate = 0.001, Optimizer = Adam, Batch Size = 32, Validation Accuracy = 99.38%\n",
        "**Epoch 30/30 -**\n",
        "|| loss: 0.0208 || acc: 0.9939 || val_loss: 0.0215 || **val_acc: 0.9938**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJTpGlgkyj1h",
        "colab_type": "text"
      },
      "source": [
        "### Initial LR = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX86TFKWylFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ee23a50-5bfb-4d3b-c4b8-6cdc1a8dc828"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=32, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 48s 802us/step - loss: 0.2001 - acc: 0.9501 - val_loss: 0.0855 - val_acc: 0.9737\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 45s 743us/step - loss: 0.0865 - acc: 0.9757 - val_loss: 0.0485 - val_acc: 0.9838\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 44s 738us/step - loss: 0.0697 - acc: 0.9796 - val_loss: 0.0502 - val_acc: 0.9839\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 44s 739us/step - loss: 0.0594 - acc: 0.9828 - val_loss: 0.0307 - val_acc: 0.9898\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 44s 740us/step - loss: 0.0538 - acc: 0.9845 - val_loss: 0.0392 - val_acc: 0.9875\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 44s 740us/step - loss: 0.0471 - acc: 0.9852 - val_loss: 0.0288 - val_acc: 0.9909\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 44s 730us/step - loss: 0.0442 - acc: 0.9870 - val_loss: 0.0315 - val_acc: 0.9902\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 45s 747us/step - loss: 0.0392 - acc: 0.9885 - val_loss: 0.0311 - val_acc: 0.9900\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 45s 748us/step - loss: 0.0377 - acc: 0.9888 - val_loss: 0.0315 - val_acc: 0.9907\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 44s 739us/step - loss: 0.0357 - acc: 0.9894 - val_loss: 0.0269 - val_acc: 0.9919\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 44s 733us/step - loss: 0.0349 - acc: 0.9892 - val_loss: 0.0271 - val_acc: 0.9910\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 45s 743us/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.0284 - val_acc: 0.9913\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 45s 748us/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0219 - val_acc: 0.9933\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 45s 751us/step - loss: 0.0290 - acc: 0.9917 - val_loss: 0.0253 - val_acc: 0.9924\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 44s 740us/step - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0264 - val_acc: 0.9920\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0260 - acc: 0.9920 - val_loss: 0.0255 - val_acc: 0.9917\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 45s 744us/step - loss: 0.0270 - acc: 0.9920 - val_loss: 0.0300 - val_acc: 0.9910\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 45s 748us/step - loss: 0.0241 - acc: 0.9928 - val_loss: 0.0318 - val_acc: 0.9909\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 43s 723us/step - loss: 0.0258 - acc: 0.9919 - val_loss: 0.0257 - val_acc: 0.9913\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 45s 743us/step - loss: 0.0222 - acc: 0.9930 - val_loss: 0.0256 - val_acc: 0.9910\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 45s 745us/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.0287 - val_acc: 0.9910\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 43s 725us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0243 - val_acc: 0.9919\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 44s 738us/step - loss: 0.0217 - acc: 0.9932 - val_loss: 0.0248 - val_acc: 0.9932\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 45s 748us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.0248 - val_acc: 0.9923\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 44s 736us/step - loss: 0.0206 - acc: 0.9932 - val_loss: 0.0292 - val_acc: 0.9914\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.0249 - val_acc: 0.9919\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 44s 734us/step - loss: 0.0195 - acc: 0.9941 - val_loss: 0.0275 - val_acc: 0.9923\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 44s 727us/step - loss: 0.0191 - acc: 0.9942 - val_loss: 0.0254 - val_acc: 0.9923\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 43s 712us/step - loss: 0.0190 - acc: 0.9936 - val_loss: 0.0260 - val_acc: 0.9922\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 43s 713us/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0269 - val_acc: 0.9926\n",
            "[0.026918109806999563, 0.9926]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjsW4TJzyrwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "309f9b0b-7789-4b88-e2d0-de89e0816dd8"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=100, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.1922 - acc: 0.9616 - val_loss: 2.1665 - val_acc: 0.1070\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0657 - acc: 0.9821 - val_loss: 0.0488 - val_acc: 0.9857\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.0535 - acc: 0.9845 - val_loss: 0.0448 - val_acc: 0.9858\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 18s 308us/step - loss: 0.0457 - acc: 0.9867 - val_loss: 0.0403 - val_acc: 0.9879\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0431 - acc: 0.9870 - val_loss: 0.0307 - val_acc: 0.9907\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.0384 - acc: 0.9886 - val_loss: 0.0259 - val_acc: 0.9931\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.0295 - val_acc: 0.9912\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.0325 - acc: 0.9907 - val_loss: 0.0292 - val_acc: 0.9913\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.0316 - acc: 0.9899 - val_loss: 0.0369 - val_acc: 0.9884\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.0295 - acc: 0.9911 - val_loss: 0.0279 - val_acc: 0.9921\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0284 - acc: 0.9914 - val_loss: 0.0297 - val_acc: 0.9901\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0262 - acc: 0.9918 - val_loss: 0.0270 - val_acc: 0.9909\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0246 - acc: 0.9925 - val_loss: 0.0303 - val_acc: 0.9910\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.0237 - acc: 0.9927 - val_loss: 0.0300 - val_acc: 0.9902\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0284 - val_acc: 0.9914\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.0278 - val_acc: 0.9912\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 17s 292us/step - loss: 0.0202 - acc: 0.9937 - val_loss: 0.0266 - val_acc: 0.9922\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0256 - val_acc: 0.9920\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 17s 283us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0317 - val_acc: 0.9908\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0181 - acc: 0.9943 - val_loss: 0.0281 - val_acc: 0.9917\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.0161 - acc: 0.9952 - val_loss: 0.0329 - val_acc: 0.9907\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0166 - acc: 0.9944 - val_loss: 0.0287 - val_acc: 0.9917\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0164 - acc: 0.9948 - val_loss: 0.0276 - val_acc: 0.9917\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0574 - val_acc: 0.9842\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 0.0292 - val_acc: 0.9913\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0344 - val_acc: 0.9900\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0146 - acc: 0.9951 - val_loss: 0.0212 - val_acc: 0.9938\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0286 - val_acc: 0.9916\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 16s 268us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0264 - val_acc: 0.9915\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0315 - val_acc: 0.9910\n",
            "[0.03145998275963357, 0.991]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dros2PrD0-jO",
        "colab_type": "text"
      },
      "source": [
        "### Initial LR = 0.005"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWbG0FL0MLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5673a6a0-7b3d-415a-fb38-a6b7fb38ffce"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=32, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 48s 801us/step - loss: 0.2220 - acc: 0.9502 - val_loss: 0.0763 - val_acc: 0.9781\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 43s 710us/step - loss: 0.0938 - acc: 0.9749 - val_loss: 0.0483 - val_acc: 0.9860\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 43s 714us/step - loss: 0.0690 - acc: 0.9810 - val_loss: 0.0589 - val_acc: 0.9831\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0591 - acc: 0.9832 - val_loss: 0.0402 - val_acc: 0.9881\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 43s 714us/step - loss: 0.0521 - acc: 0.9852 - val_loss: 0.0442 - val_acc: 0.9867\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 43s 711us/step - loss: 0.0462 - acc: 0.9867 - val_loss: 0.0343 - val_acc: 0.9901\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 42s 706us/step - loss: 0.0430 - acc: 0.9878 - val_loss: 0.0396 - val_acc: 0.9875\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 42s 708us/step - loss: 0.0391 - acc: 0.9883 - val_loss: 0.0299 - val_acc: 0.9922\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 44s 734us/step - loss: 0.0357 - acc: 0.9894 - val_loss: 0.0341 - val_acc: 0.9896\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 43s 708us/step - loss: 0.0343 - acc: 0.9896 - val_loss: 0.0268 - val_acc: 0.9922\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 43s 711us/step - loss: 0.0346 - acc: 0.9894 - val_loss: 0.0288 - val_acc: 0.9910\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 43s 713us/step - loss: 0.0313 - acc: 0.9905 - val_loss: 0.0359 - val_acc: 0.9897\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 43s 720us/step - loss: 0.0290 - acc: 0.9912 - val_loss: 0.0328 - val_acc: 0.9896\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 44s 726us/step - loss: 0.0275 - acc: 0.9918 - val_loss: 0.0276 - val_acc: 0.9921\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0273 - acc: 0.9919 - val_loss: 0.0275 - val_acc: 0.9911\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0253 - acc: 0.9923 - val_loss: 0.0309 - val_acc: 0.9910\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 42s 707us/step - loss: 0.0252 - acc: 0.9923 - val_loss: 0.0237 - val_acc: 0.9929\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 43s 718us/step - loss: 0.0236 - acc: 0.9929 - val_loss: 0.0225 - val_acc: 0.9934\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 44s 726us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.0233 - val_acc: 0.9930\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 43s 716us/step - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0264 - val_acc: 0.9921\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 44s 732us/step - loss: 0.0233 - acc: 0.9929 - val_loss: 0.0214 - val_acc: 0.9932\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 43s 724us/step - loss: 0.0213 - acc: 0.9934 - val_loss: 0.0305 - val_acc: 0.9907\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 43s 711us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0267 - val_acc: 0.9916\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 44s 726us/step - loss: 0.0226 - acc: 0.9934 - val_loss: 0.0226 - val_acc: 0.9930\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 43s 721us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0238 - val_acc: 0.9931\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 43s 717us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0254 - val_acc: 0.9929\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 43s 710us/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0204 - val_acc: 0.9938\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0236 - val_acc: 0.9923\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 42s 702us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0318 - val_acc: 0.9912\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 43s 708us/step - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0291 - val_acc: 0.9912\n",
            "[0.029103648282471112, 0.9912]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf6OgwxC1uLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ab8148e-53cd-4e53-9aae-8ad97a2ff132"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=100, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.2526 - acc: 0.9556 - val_loss: 0.0663 - val_acc: 0.9845\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 17s 286us/step - loss: 0.0729 - acc: 0.9835 - val_loss: 0.0379 - val_acc: 0.9899\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0547 - acc: 0.9864 - val_loss: 0.0456 - val_acc: 0.9864\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.0478 - acc: 0.9872 - val_loss: 0.0384 - val_acc: 0.9884\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0411 - acc: 0.9888 - val_loss: 0.0443 - val_acc: 0.9871\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0373 - acc: 0.9891 - val_loss: 0.0303 - val_acc: 0.9917\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 17s 286us/step - loss: 0.0347 - acc: 0.9899 - val_loss: 0.0340 - val_acc: 0.9905\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 17s 287us/step - loss: 0.0318 - acc: 0.9904 - val_loss: 0.0267 - val_acc: 0.9920\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0308 - acc: 0.9908 - val_loss: 0.0281 - val_acc: 0.9906\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.0288 - acc: 0.9915 - val_loss: 0.0264 - val_acc: 0.9918\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0259 - acc: 0.9928 - val_loss: 0.0314 - val_acc: 0.9905\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0250 - acc: 0.9927 - val_loss: 0.0298 - val_acc: 0.9911\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0256 - val_acc: 0.9917\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 17s 283us/step - loss: 0.0228 - acc: 0.9930 - val_loss: 0.0351 - val_acc: 0.9894\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9932\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0282 - val_acc: 0.9903\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 17s 283us/step - loss: 0.0205 - acc: 0.9941 - val_loss: 0.0333 - val_acc: 0.9908\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0248 - val_acc: 0.9929\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0186 - acc: 0.9944 - val_loss: 0.0246 - val_acc: 0.9923\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.0306 - val_acc: 0.9912\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0231 - val_acc: 0.9935\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 17s 285us/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0284 - val_acc: 0.9923\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0351 - val_acc: 0.9897\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0259 - val_acc: 0.9917\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0199 - val_acc: 0.9939\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0267 - val_acc: 0.9920\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 16s 258us/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0479 - val_acc: 0.9858\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0371 - val_acc: 0.9891\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0320 - val_acc: 0.9907\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 16s 261us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0224 - val_acc: 0.9941\n",
            "[0.02240345571915677, 0.9941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTseAGfI1CsU",
        "colab_type": "text"
      },
      "source": [
        "### Initial LR = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL4jcZoO0e9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93182bb9-3993-4697-b0cf-ea2fd70fd1fd"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=32, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 45s 758us/step - loss: 0.4239 - acc: 0.9208 - val_loss: 0.1293 - val_acc: 0.9805\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 42s 702us/step - loss: 0.1587 - acc: 0.9681 - val_loss: 0.0629 - val_acc: 0.9886\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 42s 694us/step - loss: 0.1084 - acc: 0.9764 - val_loss: 0.0489 - val_acc: 0.9910\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0815 - acc: 0.9805 - val_loss: 0.0387 - val_acc: 0.9905\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 42s 697us/step - loss: 0.0707 - acc: 0.9826 - val_loss: 0.0388 - val_acc: 0.9903\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0610 - acc: 0.9846 - val_loss: 0.0427 - val_acc: 0.9887\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0540 - acc: 0.9866 - val_loss: 0.0328 - val_acc: 0.9904\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 42s 706us/step - loss: 0.0501 - acc: 0.9871 - val_loss: 0.0298 - val_acc: 0.9918\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 42s 700us/step - loss: 0.0455 - acc: 0.9885 - val_loss: 0.0253 - val_acc: 0.9920\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 43s 710us/step - loss: 0.0415 - acc: 0.9891 - val_loss: 0.0336 - val_acc: 0.9899\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0394 - acc: 0.9894 - val_loss: 0.0339 - val_acc: 0.9895\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0358 - acc: 0.9902 - val_loss: 0.0280 - val_acc: 0.9913\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0357 - acc: 0.9910 - val_loss: 0.0248 - val_acc: 0.9919\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 42s 701us/step - loss: 0.0339 - acc: 0.9905 - val_loss: 0.0257 - val_acc: 0.9916\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 43s 712us/step - loss: 0.0335 - acc: 0.9907 - val_loss: 0.0293 - val_acc: 0.9907\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 42s 696us/step - loss: 0.0313 - acc: 0.9915 - val_loss: 0.0238 - val_acc: 0.9932\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 43s 709us/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.0259 - val_acc: 0.9918\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 42s 701us/step - loss: 0.0289 - acc: 0.9921 - val_loss: 0.0246 - val_acc: 0.9929\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 42s 702us/step - loss: 0.0284 - acc: 0.9924 - val_loss: 0.0293 - val_acc: 0.9908\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 43s 709us/step - loss: 0.0275 - acc: 0.9920 - val_loss: 0.0250 - val_acc: 0.9918\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0260 - acc: 0.9930 - val_loss: 0.0255 - val_acc: 0.9924\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 42s 703us/step - loss: 0.0253 - acc: 0.9928 - val_loss: 0.0207 - val_acc: 0.9937\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 42s 701us/step - loss: 0.0256 - acc: 0.9928 - val_loss: 0.0225 - val_acc: 0.9930\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 42s 701us/step - loss: 0.0248 - acc: 0.9931 - val_loss: 0.0231 - val_acc: 0.9936\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 43s 719us/step - loss: 0.0231 - acc: 0.9931 - val_loss: 0.0214 - val_acc: 0.9940\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 44s 738us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0233 - val_acc: 0.9932\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 44s 740us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0241 - val_acc: 0.9931\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 44s 739us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0222 - val_acc: 0.9931\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 44s 733us/step - loss: 0.0211 - acc: 0.9939 - val_loss: 0.0202 - val_acc: 0.9944\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 43s 724us/step - loss: 0.0208 - acc: 0.9939 - val_loss: 0.0215 - val_acc: 0.9938\n",
            "[0.02146430257386528, 0.9938]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUM0rGzO1xbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9dac1ac0-848b-4412-aa41-2b83b62c8ba3"
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=adam,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.load_weights('model_BN_AND_DROPOUT.h5')\n",
        "model.fit(X_train, Y_train, batch_size=100, \n",
        "          epochs=30, verbose=1, \n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.5233 - acc: 0.9133 - val_loss: 0.2051 - val_acc: 0.9784\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.1880 - acc: 0.9774 - val_loss: 0.1199 - val_acc: 0.9860\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 17s 280us/step - loss: 0.1185 - acc: 0.9831 - val_loss: 0.1083 - val_acc: 0.9839\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0872 - acc: 0.9855 - val_loss: 0.0711 - val_acc: 0.9878\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0695 - acc: 0.9877 - val_loss: 0.0506 - val_acc: 0.9903\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0588 - acc: 0.9882 - val_loss: 0.0477 - val_acc: 0.9896\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0501 - acc: 0.9896 - val_loss: 0.0374 - val_acc: 0.9914\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0442 - acc: 0.9903 - val_loss: 0.0365 - val_acc: 0.9911\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0416 - acc: 0.9905 - val_loss: 0.0423 - val_acc: 0.9886\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 16s 270us/step - loss: 0.0373 - acc: 0.9910 - val_loss: 0.0272 - val_acc: 0.9920\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 16s 269us/step - loss: 0.0337 - acc: 0.9916 - val_loss: 0.0255 - val_acc: 0.9933\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 16s 269us/step - loss: 0.0307 - acc: 0.9922 - val_loss: 0.0291 - val_acc: 0.9924\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0285 - acc: 0.9926 - val_loss: 0.0336 - val_acc: 0.9911\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0274 - acc: 0.9932 - val_loss: 0.0339 - val_acc: 0.9902\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0258 - acc: 0.9931 - val_loss: 0.0265 - val_acc: 0.9930\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0250 - acc: 0.9932 - val_loss: 0.0242 - val_acc: 0.9927\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0237 - acc: 0.9937 - val_loss: 0.0267 - val_acc: 0.9927\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0232 - acc: 0.9938 - val_loss: 0.0255 - val_acc: 0.9924\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 16s 262us/step - loss: 0.0216 - acc: 0.9940 - val_loss: 0.0245 - val_acc: 0.9926\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0219 - acc: 0.9939 - val_loss: 0.0233 - val_acc: 0.9931\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0277 - val_acc: 0.9916\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.0270 - val_acc: 0.9924\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0191 - acc: 0.9946 - val_loss: 0.0272 - val_acc: 0.9917\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0236 - val_acc: 0.9925\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0287 - val_acc: 0.9918\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0238 - val_acc: 0.9919\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0250 - val_acc: 0.9922\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0160 - acc: 0.9957 - val_loss: 0.0242 - val_acc: 0.9918\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0247 - val_acc: 0.9922\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0331 - val_acc: 0.9901\n",
            "[0.03308991756620817, 0.9901]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXaTb_Q29fDA",
        "colab_type": "text"
      },
      "source": [
        "## Best Validation Accuracy Achieved: 99.42%\n",
        "\n",
        "###With Following Parameters:\n",
        "1. Batch Normalisation\n",
        "2. Dropout = 0.15\n",
        "3. Optimizer = Adam [Default Learning Rate lr=0.001]\n",
        "4. Batch Size=32\n",
        "5. Epoch = 17\n",
        "6. **Total params**: 14,024, \n",
        "7. **Trainable params**: 13,760, **Non-trainable params**: 264\n",
        "8. Validation loss: 2.33%"
      ]
    }
  ]
}