{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LAG_jJ-Q9yfA",
        "t-OUbDlz5SrI",
        "olUduP0c3twf",
        "-OFI9MLr9R_J",
        "EJmdaHBE3jYI",
        "QdwvhxrE3mHj",
        "-6Ln6fP-kHHH",
        "B0FPNesq34Pg",
        "KuEctVc037sw",
        "AFFHQuYR4BRf",
        "gfsa0PWZoaq-",
        "5BhtD4xFwi3Y",
        "_hfZmUWs8nn2",
        "ozUBl5SrzvdA",
        "IjaMl4yD2i4c",
        "qmCT_yfH2ad_",
        "xSL2Ziz30BH9",
        "lYu-gxP73F8J",
        "TbSnBe8JjcKd",
        "-dzawOAGJiCR",
        "7871EH6hCvfT",
        "4DDsr6Nd7Um4",
        "POButtmYLv5G",
        "VVvgl2ecLm8O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAG_jJ-Q9yfA",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 5**:\n",
        "**Change the code 8 or your own 4th Code from Assignment 4 to include:**\n",
        "1. image normalization\n",
        "2. L2 regularization\n",
        "3. ReLU after BN\n",
        "\n",
        "* Run your new code for 40 epochs and save the model with highest validation accuracy\n",
        "* Find out 25 misclassified images from the validation dataset and create an image gallery\n",
        "* Submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GbiWJRc5BMk",
        "colab_type": "text"
      },
      "source": [
        "### **Import Libraries and modules**\n",
        "1. import the basic libraries\n",
        "2. import libraries required for building up the model using keras\n",
        "3. import the mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJyVpgSxHt4",
        "colab_type": "code",
        "outputId": "742e2e0e-23e1-42f1-b2bb-79794ba02aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4NysoAn5DoV",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets\n",
        "Load data from mnist dataset and split the data between training dataset and testing dataset.\n",
        "\n",
        "###**Print and Plot**:\n",
        "1. Print the shape for the trainig dataset\n",
        "2. Import pyplot for plotting the input image\n",
        "3. Plot the Input Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Y9Va-xxMXG",
        "colab_type": "code",
        "outputId": "7af59139-9746-47e4-e0b3-00a95a3df964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f70e8966320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-OUbDlz5SrI",
        "colab_type": "text"
      },
      "source": [
        "#**Model Creation Using Keras**\n",
        "The following code defines the architecture/model of our neural network in which we will add layers in the network using keras:\n",
        "1.   **Initializing a sequential model**: Stack layers on top of one another, it also ensures that the input and output sizes of successive layers are compatible. \n",
        "2.   **Adding Convolution Layers**: While adding the first convolution layer we need to specify the input size as well. In the next convolution layers we do not need an input size since they will receive the input size from the preceeding layers.\n",
        "3. **Adding Max Pooling Layers**: We add them to resuce the dimensions of image extracting the maximum out of the (2,2) pool and pass them forward.\n",
        "4. **Adding Flatten Layer**: When finally we reach the output size of 1X1X10, we flatten the output channel-wise into output of shape 10 (1-dimension).\n",
        "5. **Adding Softmax Layer**: This layer converts the last layer outuput into probabolity distribution of 10 classes.\n",
        "\n",
        "## DNN - Batch Normalization and Dropout\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Batch Normalization\n",
        "> It is used to normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. \n",
        "\n",
        "> Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the **internal covariate shift** problem.\n",
        "Batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.\n",
        "\n",
        "> **Internal covariate shift** -  Each layer must learn to adapt themselves to a new distribution in every training step which slows down the training process.\n",
        "\n",
        "**Steps for BN:**\n",
        "1. Calculate the mean and variance of the layers input.\n",
        "2. Normalize the layer inputs using the previously calculated batch statistics.\n",
        "3. Scale and shift in order to obtain the output of the layer.\n",
        "**P.S - γ and β are learned during training along with the original parameters of the network.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###Dropout\n",
        "> The term “Dropout” refers to dropping out units (both hidden and visible) in a neural network. Dropout ignores units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, I mean these units are not considered during a particular forward or backward pass.\n",
        "\n",
        "> Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.During training, some number of layer outputs are randomly ignored or “dropped out.”\n",
        "####Why do we need Dropout?\n",
        "\n",
        "> To prevent over-fitting\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Total params: 16,604\n",
        "\n",
        "Trainable params: 16,360\n",
        "\n",
        "Non-trainable params: 244\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olUduP0c3twf",
        "colab_type": "text"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpXf4YQxXRm",
        "colab_type": "code",
        "outputId": "9e3b0e15-21e0-481b-806a-0f2aee77bd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########################################\n",
        "model = Sequential()\n",
        "########################################\n",
        "\n",
        "# All shapes are in the form [height, width, num_channels]\n",
        "\n",
        "# INPUT SIZE : [28, 28, 1] | OUTPUT SIZE : [26, 26, 16] | GRF_of_input : [1, 1]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [26, 26, 16] | OUTPUT SIZE : [24, 24, 32] | GRF_of_input : [3, 3]\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [24, 24, 32] | OUTPUT SIZE : [24, 24, 10] | GRF_of_input : [5, 5]\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu'))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [24, 24, 10] | OUTPUT SIZE : [12, 12, 10] | GRF_of_input : [5, 5]\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "########################################\n",
        "# INPUT SIZE : [12, 12, 10] | OUTPUT SIZE : [10, 10, 16] | GRF_of_input : [10, 10]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "# INPUT SIZE : [10, 10, 16] | OUTPUT SIZE : [8, 8, 16] | GRF_of_input : [12, 12]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "# INPUT SIZE : [8, 8, 16] | OUTPUT SIZE : [6, 6, 16] | GRF_of_input : [14, 14]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "# INPUT SIZE : [6, 6, 16] | OUTPUT SIZE : [4, 4, 16] | GRF_of_input : [16, 16]\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "# INPUT SIZE : [4, 4, 16] | OUTPUT SIZE : [1, 1, 10] | GRF_of_input : [18, 18]\n",
        "model.add(Convolution2D(10, 4, 4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [1, 1, 10] | OUTPUT SIZE : [10] | GRF_of_input : [22, 22]\n",
        "model.add(Flatten())\n",
        "\n",
        "# INPUT SIZE : [10] | OUTPUT SIZE : [10] | GRF_of_input : [22, 22]\n",
        "model.add(Activation('softmax'))\n",
        "########################################\n",
        "\n",
        "# Printing out the model summary\n",
        "model.summary()\n",
        "\n",
        "# saving the model to reload model with initial weights while trying out different optimizations\n",
        "model.save_weights('model.h5')\n",
        "########################################"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,604\n",
            "Trainable params: 16,360\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFI9MLr9R_J",
        "colab_type": "text"
      },
      "source": [
        "### Defining LR Scheduler\n",
        "\n",
        "**Learning Rate**\n",
        "The amount of change to the model during each step of this search process, or the step size, is called the “learning rate”. \n",
        "\n",
        "Learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
        "\n",
        "During training, the backpropagation of error estimates the amount of error for which the weights of a node in the network are responsible. Instead of updating the weight with the full amount, it is scaled by the learning rate.\n",
        "\n",
        "Learning rate controls how quickly or slowly a neural network model learns a problem.\n",
        "\n",
        "> **Large learning rate** allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights.\n",
        "\n",
        "> **Small learning rate** may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.\n",
        "\n",
        "### Need for Learning Rate Schedules\n",
        "**Benefits:**\n",
        "1. Converge faster\n",
        "2. Higher accuracy\n",
        "\n",
        "---\n",
        "\n",
        "**We tried our model with 3 initial learning rate:**\n",
        "1. LR-0.003, epochs=20, Validation Accuracy = 99.46%\n",
        "2. LR-0.01, epochs=20, Validation Accuracy = 99.42%\n",
        "3. LR-0.1, epochs=30, Validation Accuracy = 99.44%\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbkflE-K9ZYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "#########################################################################\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(lr * 1/(1 + 0.319 * epoch), 10)\n",
        "#########################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJmdaHBE3jYI",
        "colab_type": "text"
      },
      "source": [
        "#### LR - 0.003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IicGJ4x3Be",
        "colab_type": "code",
        "outputId": "74ca5a03-1f89-4858-a0e8-02a7b3844d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#########################################################################\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit(X_train, Y_train, \n",
        "          batch_size=128, \n",
        "          epochs=20, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, Y_test), \n",
        "          callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################\n",
        "# [0.019468824124510866, 0.9946] LR - 0.003; epochs 20; \n",
        "# [0.019800396383114274, 0.9942] LR - 0.01; epochs 20; [0.01, 0.0075815011, 0.00141623 ]\n",
        "# [0.019371943644070144, 0.9944] LR - 0.1; epochs 30; [0.1, 0.0758150114, 0.0097551458 ] loss: 0.0823 - acc: 0.9571 - val_loss: 0.0194 - val_acc: 0.9944"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 14s 229us/step - loss: 0.5032 - acc: 0.8595 - val_loss: 0.0883 - val_acc: 0.9846\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.2495 - acc: 0.9268 - val_loss: 0.0592 - val_acc: 0.9871\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1962 - acc: 0.9415 - val_loss: 0.0447 - val_acc: 0.9901\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1644 - acc: 0.9490 - val_loss: 0.0346 - val_acc: 0.9912\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1490 - acc: 0.9505 - val_loss: 0.0338 - val_acc: 0.9919\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1386 - acc: 0.9504 - val_loss: 0.0294 - val_acc: 0.9925\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1277 - acc: 0.9525 - val_loss: 0.0282 - val_acc: 0.9925\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1199 - acc: 0.9540 - val_loss: 0.0264 - val_acc: 0.9921\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1154 - acc: 0.9552 - val_loss: 0.0255 - val_acc: 0.9927\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1117 - acc: 0.9554 - val_loss: 0.0229 - val_acc: 0.9930\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.1083 - acc: 0.9549 - val_loss: 0.0230 - val_acc: 0.9938\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.1050 - acc: 0.9558 - val_loss: 0.0203 - val_acc: 0.9944\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.1021 - acc: 0.9544 - val_loss: 0.0193 - val_acc: 0.9940\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.1013 - acc: 0.9570 - val_loss: 0.0201 - val_acc: 0.9946\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0960 - acc: 0.9585 - val_loss: 0.0192 - val_acc: 0.9944\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0962 - acc: 0.9575 - val_loss: 0.0206 - val_acc: 0.9940\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0918 - acc: 0.9590 - val_loss: 0.0199 - val_acc: 0.9941\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0957 - acc: 0.9568 - val_loss: 0.0191 - val_acc: 0.9940\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0905 - acc: 0.9586 - val_loss: 0.0183 - val_acc: 0.9944\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.0903 - acc: 0.9581 - val_loss: 0.0195 - val_acc: 0.9946\n",
            "[0.019468824124510866, 0.9946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdwvhxrE3mHj",
        "colab_type": "text"
      },
      "source": [
        "#### LR - 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iOvpWMcdqbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23d94356-f724-4816-b650-1bc3d46622e8"
      },
      "source": [
        "#########################################################################\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.1), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit(X_train, Y_train, \n",
        "          batch_size=128, \n",
        "          epochs=30, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, Y_test), \n",
        "          callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.3117 - acc: 0.8919 - val_loss: 0.0857 - val_acc: 0.9748\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0758150114.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.1613 - acc: 0.9365 - val_loss: 0.0654 - val_acc: 0.9796\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0610500611.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1386 - acc: 0.9441 - val_loss: 0.0607 - val_acc: 0.9832\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0510986203.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1256 - acc: 0.9470 - val_loss: 0.0491 - val_acc: 0.9855\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0439367311.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.1203 - acc: 0.9481 - val_loss: 0.0396 - val_acc: 0.9883\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0385356455.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1155 - acc: 0.9497 - val_loss: 0.0403 - val_acc: 0.9878\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0343170899.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1122 - acc: 0.9492 - val_loss: 0.0374 - val_acc: 0.9886\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0309310238.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1072 - acc: 0.9516 - val_loss: 0.0246 - val_acc: 0.9928\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0281531532.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.1055 - acc: 0.9520 - val_loss: 0.0328 - val_acc: 0.9900\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0258331181.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.1025 - acc: 0.9515 - val_loss: 0.0278 - val_acc: 0.9921\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0238663484.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0994 - acc: 0.9540 - val_loss: 0.0238 - val_acc: 0.9931\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0221778665.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0994 - acc: 0.9531 - val_loss: 0.0252 - val_acc: 0.9927\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0207125104.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0964 - acc: 0.9545 - val_loss: 0.0299 - val_acc: 0.9916\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0194287935.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0964 - acc: 0.9536 - val_loss: 0.0216 - val_acc: 0.9942\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.018294914.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0933 - acc: 0.9549 - val_loss: 0.0270 - val_acc: 0.9928\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0172860847.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0934 - acc: 0.9554 - val_loss: 0.0238 - val_acc: 0.9930\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0163826999.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0888 - acc: 0.9551 - val_loss: 0.0254 - val_acc: 0.9931\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0155690487.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0904 - acc: 0.9557 - val_loss: 0.0303 - val_acc: 0.9917\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0148323939.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0892 - acc: 0.9554 - val_loss: 0.0239 - val_acc: 0.9932\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0141623.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0874 - acc: 0.9567 - val_loss: 0.0226 - val_acc: 0.9937\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0135501355.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0852 - acc: 0.9566 - val_loss: 0.0237 - val_acc: 0.9937\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0129886998.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0858 - acc: 0.9563 - val_loss: 0.0205 - val_acc: 0.9944\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0124719381.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0838 - acc: 0.9578 - val_loss: 0.0218 - val_acc: 0.9939\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0119947223.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0854 - acc: 0.9584 - val_loss: 0.0235 - val_acc: 0.9945\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0115526802.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0831 - acc: 0.9578 - val_loss: 0.0245 - val_acc: 0.9939\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0111420613.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0803 - acc: 0.9596 - val_loss: 0.0210 - val_acc: 0.9945\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0107596299.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0820 - acc: 0.9586 - val_loss: 0.0255 - val_acc: 0.9936\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0104025798.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0828 - acc: 0.9569 - val_loss: 0.0206 - val_acc: 0.9943\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0100684656.\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0802 - acc: 0.9590 - val_loss: 0.0254 - val_acc: 0.9933\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0097551458.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0823 - acc: 0.9571 - val_loss: 0.0194 - val_acc: 0.9944\n",
            "[0.019371943644070144, 0.9944]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Ln6fP-kHHH",
        "colab_type": "text"
      },
      "source": [
        "# Using Image Normalization\n",
        "\n",
        "##Normalizing image inputs\n",
        "* Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. \n",
        "* This makes convergence faster while training the network. \n",
        "* Data normalization is done by subtracting the mean from each pixel and then dividing the result by the standard deviation.\n",
        "* The distribution of such data would resemble a Gaussian curve centered at zero.\n",
        "* For image inputs we need the pixel numbers to be positive, so we might choose to scale the normalized data in the range [0,1].\n",
        "* In this process for each feature to have a similar range so that our gradients don't go out of control \n",
        "\n",
        "### ImageDataGenerator\n",
        "* Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).\n",
        "* defines the configuration for image data preparation and augmentation.\n",
        "\n",
        "> #### This includes capabilities such as:\n",
        "1. Sample-wise standardization.\n",
        "2. Feature-wise standardization.\n",
        "3. ZCA whitening.\n",
        "4. Random rotation, shifts, shear and flips.\n",
        "5. Dimension reordering.\n",
        "6. Save augmented images to disk.\n",
        "\n",
        ">#### Fit\n",
        "* Fits the data generator to some sample data.\n",
        "* This computes the internal data stats related to the data-dependent transformations, based on an array of sample data.\n",
        "* Only required if featurewise_center or featurewise_std_normalization or zca_whitening are set to True.\n",
        "\n",
        ">#### Flow\n",
        "* Takes data & label arrays, generates batches of augmented data.\n",
        "* We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zga4IesR9yS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1728d69b-9a72-4ad8-e0f2-7972722ca1d9"
      },
      "source": [
        "# https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Image Normalization\n",
        "\n",
        "# Create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# Calculate the mean on the trainng dataset\n",
        "datagen.fit(X_train)\n",
        "print(\"Data Generator mean=%.3f, std=%.3f\" % (datagen.mean, datagen.std))\n",
        "\n",
        "# We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "\n",
        "#########################################################################"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY1NzldXCQJq",
        "colab_type": "text"
      },
      "source": [
        "#### We tried Normalized Image Data with 3 initial learning rate:\n",
        "1. LR-0.003, epochs=20, Validation Accuracy = 72.88%\n",
        "2. LR-0.1, epochs=30, Validation Accuracy = 79.31%\n",
        "3. LR- 1, epochs=30, Validation Accuracy = 76.60%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0FPNesq34Pg",
        "colab_type": "text"
      },
      "source": [
        "#### LR - 0.003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKM2B5ZljVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54ce407d-568b-49a2-8dc8-64663ed6d1f1"
      },
      "source": [
        "#########################################################################\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit_generator(iterator, \n",
        "                    samples_per_epoch=len(X_train), \n",
        "                    epochs=20,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test), \n",
        "                    callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "#########################################################################\n",
        "# [3.4018, 0.7288]            LR - 0.003; epochs 20; [0.003, 0.0022744503, ...  0.000424869]; loss: 0.0766 - acc: 0.9606 - val_loss: 3.4018 - val_acc: 0.7288\n",
        "# [2.930216279411316, 0.7931] LR - 0.1;   epochs 30; [0.1, 0.0758150114  0.0097551458];       loss: 0.0790 - acc: 0.9597 - val_loss: 2.9302 - val_acc: 0.7931\n",
        "# [3.512940872049332, 0.766]  LR - 1.0 ;  epochs 30; [1.0, 0.7581501137, ...  0.097551458];   loss: 0.0871 - acc: 0.9558 - val_loss: 3.5129 - val_acc: 0.7660"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "  5/468 [..............................] - ETA: 15s - loss: 0.0814 - acc: 0.9641"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=20, verbose=1, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1149 - acc: 0.9529 - val_loss: 3.0290 - val_acc: 0.7643\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1019 - acc: 0.9542 - val_loss: 2.9588 - val_acc: 0.7626\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0977 - acc: 0.9552 - val_loss: 3.0986 - val_acc: 0.7417\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0932 - acc: 0.9561 - val_loss: 2.7389 - val_acc: 0.7648\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0924 - acc: 0.9569 - val_loss: 3.2153 - val_acc: 0.7468\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0861 - acc: 0.9589 - val_loss: 2.2511 - val_acc: 0.7917\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0862 - acc: 0.9577 - val_loss: 2.5656 - val_acc: 0.7840\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0854 - acc: 0.9580 - val_loss: 3.0799 - val_acc: 0.7503\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0820 - acc: 0.9597 - val_loss: 2.4905 - val_acc: 0.8041\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0802 - acc: 0.9593 - val_loss: 2.4244 - val_acc: 0.8054\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0810 - acc: 0.9598 - val_loss: 2.8504 - val_acc: 0.7547\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0795 - acc: 0.9595 - val_loss: 3.4905 - val_acc: 0.7269\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0790 - acc: 0.9590 - val_loss: 3.2646 - val_acc: 0.7464\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0803 - acc: 0.9592 - val_loss: 3.1049 - val_acc: 0.7438\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0791 - acc: 0.9594 - val_loss: 3.6459 - val_acc: 0.7160\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0792 - acc: 0.9601 - val_loss: 3.2966 - val_acc: 0.7322\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0778 - acc: 0.9601 - val_loss: 2.9981 - val_acc: 0.7581\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0765 - acc: 0.9609 - val_loss: 3.9133 - val_acc: 0.7020\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0764 - acc: 0.9610 - val_loss: 3.8440 - val_acc: 0.7065\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0766 - acc: 0.9606 - val_loss: 3.4018 - val_acc: 0.7288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70e70930b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuEctVc037sw",
        "colab_type": "text"
      },
      "source": [
        "#### LR - 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2qIfML2e8Nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d13e602b-fc2f-4d3b-a875-1ba26153870e"
      },
      "source": [
        "#########################################################################\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.1), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit_generator(iterator, \n",
        "                    samples_per_epoch=len(X_train), \n",
        "                    epochs=30,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test), \n",
        "                    callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################\n",
        "# [2.930216279411316, 0.7931] LR - 0.1; epochs 30; [0.1, 0.0758150114  0.0097551458]; loss: 0.0790 - acc: 0.9597 - val_loss: 2.9302 - val_acc: 0.7931\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=30, verbose=1, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
            "468/468 [==============================] - 19s 40ms/step - loss: 0.3236 - acc: 0.8886 - val_loss: 4.1423 - val_acc: 0.7236\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0758150114.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1594 - acc: 0.9374 - val_loss: 4.9719 - val_acc: 0.6520\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0610500611.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1365 - acc: 0.9430 - val_loss: 4.6084 - val_acc: 0.6851\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0510986203.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1263 - acc: 0.9469 - val_loss: 4.1723 - val_acc: 0.7232\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0439367311.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1147 - acc: 0.9493 - val_loss: 5.2883 - val_acc: 0.6501\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0385356455.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1150 - acc: 0.9490 - val_loss: 3.5773 - val_acc: 0.7512\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0343170899.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1077 - acc: 0.9519 - val_loss: 4.1424 - val_acc: 0.7060\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0309310238.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1052 - acc: 0.9508 - val_loss: 3.1797 - val_acc: 0.7855\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0281531532.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1019 - acc: 0.9534 - val_loss: 3.9287 - val_acc: 0.7263\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0258331181.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1032 - acc: 0.9528 - val_loss: 2.7467 - val_acc: 0.8012\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0238663484.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0965 - acc: 0.9546 - val_loss: 2.8592 - val_acc: 0.7980\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0221778665.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0961 - acc: 0.9534 - val_loss: 3.3785 - val_acc: 0.7743\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0207125104.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0965 - acc: 0.9536 - val_loss: 3.9006 - val_acc: 0.7303\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0194287935.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0914 - acc: 0.9564 - val_loss: 3.9010 - val_acc: 0.7336\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.018294914.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0953 - acc: 0.9533 - val_loss: 3.7963 - val_acc: 0.7423\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0172860847.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0891 - acc: 0.9570 - val_loss: 3.3774 - val_acc: 0.7737\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0163826999.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0914 - acc: 0.9550 - val_loss: 3.3779 - val_acc: 0.7680\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0155690487.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0905 - acc: 0.9562 - val_loss: 3.6608 - val_acc: 0.7480\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0148323939.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0864 - acc: 0.9565 - val_loss: 4.4807 - val_acc: 0.7026\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0141623.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0839 - acc: 0.9578 - val_loss: 4.8159 - val_acc: 0.6865\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0135501355.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0855 - acc: 0.9567 - val_loss: 5.4801 - val_acc: 0.6368\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0129886998.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0861 - acc: 0.9580 - val_loss: 4.3844 - val_acc: 0.7138\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0124719381.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0850 - acc: 0.9568 - val_loss: 2.6522 - val_acc: 0.8227\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0119947223.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0833 - acc: 0.9587 - val_loss: 3.0056 - val_acc: 0.7931\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0115526802.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0802 - acc: 0.9593 - val_loss: 3.0692 - val_acc: 0.7923\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0111420613.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0806 - acc: 0.9588 - val_loss: 2.8026 - val_acc: 0.8093\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0107596299.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0818 - acc: 0.9578 - val_loss: 2.9281 - val_acc: 0.8013\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0104025798.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0810 - acc: 0.9580 - val_loss: 3.8624 - val_acc: 0.7244\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0100684656.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0776 - acc: 0.9587 - val_loss: 3.1102 - val_acc: 0.7860\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0097551458.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0790 - acc: 0.9597 - val_loss: 2.9302 - val_acc: 0.7931\n",
            "[2.930216279411316, 0.7931]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFFHQuYR4BRf",
        "colab_type": "text"
      },
      "source": [
        "#### LR - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aldgTD6ve9Fj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4eab477b-2b3c-4d5d-a521-0b4c64ebc3b3"
      },
      "source": [
        "#########################################################################\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1.0), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit_generator(iterator, \n",
        "                    samples_per_epoch=len(X_train), \n",
        "                    epochs=30,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test), \n",
        "                    callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################\n",
        "# [3.512940872049332, 0.766] LR - 1.0 ; epochs 30; [1.0, 0.7581501137, ...  0.097551458]; loss: 0.0871 - acc: 0.9558 - val_loss: 3.5129 - val_acc: 0.7660"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=30, verbose=1, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 1.0.\n",
            "468/468 [==============================] - 19s 41ms/step - loss: 0.4182 - acc: 0.8579 - val_loss: 12.1804 - val_acc: 0.2379\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.7581501137.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1981 - acc: 0.9269 - val_loss: 6.7029 - val_acc: 0.5585\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.6105006105.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1666 - acc: 0.9364 - val_loss: 6.7428 - val_acc: 0.5579\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.5109862034.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1482 - acc: 0.9407 - val_loss: 5.6320 - val_acc: 0.6296\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.4393673111.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1413 - acc: 0.9411 - val_loss: 4.9566 - val_acc: 0.6746\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.3853564547.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1349 - acc: 0.9437 - val_loss: 7.2761 - val_acc: 0.5253\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.3431708991.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1241 - acc: 0.9477 - val_loss: 7.0470 - val_acc: 0.5448\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.3093102382.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1207 - acc: 0.9477 - val_loss: 7.2057 - val_acc: 0.5331\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.2815315315.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1146 - acc: 0.9491 - val_loss: 5.0302 - val_acc: 0.6604\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.2583311806.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1136 - acc: 0.9497 - val_loss: 7.4394 - val_acc: 0.5238\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.2386634845.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1139 - acc: 0.9492 - val_loss: 5.7742 - val_acc: 0.6272\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.2217786649.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1105 - acc: 0.9499 - val_loss: 6.1259 - val_acc: 0.6025\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.2071251036.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1052 - acc: 0.9516 - val_loss: 4.2684 - val_acc: 0.7182\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.1942879347.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1057 - acc: 0.9519 - val_loss: 4.9546 - val_acc: 0.6725\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.1829491401.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1047 - acc: 0.9515 - val_loss: 5.8183 - val_acc: 0.6192\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.172860847.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.1032 - acc: 0.9527 - val_loss: 7.3604 - val_acc: 0.5245\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.1638269987.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.1020 - acc: 0.9524 - val_loss: 4.8538 - val_acc: 0.6788\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.1556904873.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0988 - acc: 0.9544 - val_loss: 4.5335 - val_acc: 0.6995\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.1483239395.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0959 - acc: 0.9547 - val_loss: 3.9386 - val_acc: 0.7331\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.1416229996.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0969 - acc: 0.9543 - val_loss: 4.9414 - val_acc: 0.6731\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.135501355.\n",
            "468/468 [==============================] - 13s 29ms/step - loss: 0.0939 - acc: 0.9543 - val_loss: 4.3425 - val_acc: 0.7123\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.1298869983.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0910 - acc: 0.9560 - val_loss: 5.7376 - val_acc: 0.6213\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.1247193814.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0916 - acc: 0.9545 - val_loss: 6.0361 - val_acc: 0.6021\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.1199472232.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0899 - acc: 0.9553 - val_loss: 5.0342 - val_acc: 0.6688\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.1155268022.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0867 - acc: 0.9570 - val_loss: 4.1158 - val_acc: 0.7204\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.1114206128.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0895 - acc: 0.9576 - val_loss: 4.1792 - val_acc: 0.7168\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.1075962987.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0890 - acc: 0.9553 - val_loss: 3.9413 - val_acc: 0.7300\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.1040257984.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0859 - acc: 0.9582 - val_loss: 3.8527 - val_acc: 0.7351\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.1006846557.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0862 - acc: 0.9563 - val_loss: 4.8038 - val_acc: 0.6772\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0975514584.\n",
            "468/468 [==============================] - 13s 28ms/step - loss: 0.0871 - acc: 0.9558 - val_loss: 3.5129 - val_acc: 0.7660\n",
            "[3.512940872049332, 0.766]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfsa0PWZoaq-",
        "colab_type": "text"
      },
      "source": [
        "# Applying L2 regularization\n",
        "\n",
        "### Regularization\n",
        "* This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
        "* Regularization, significantly reduces the variance of the model, without substantial increase in its bias.\n",
        "* Regularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function.\n",
        "* Regularization works on assumption that smaller weights generate simpler model and thus helps avoid overfitting.\n",
        "\n",
        "####Two among many Regularization techniques commonly used are:\n",
        "1. **L1 Regularization [Lasso Regression]** - Least Absolute Shrinkage and Selection Operator adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "<figure>\n",
        "<img src='https://miro.medium.com/max/267/1*jgWOhDiGjVp-NCSPa5abmg.png' />\n",
        "</figure>\n",
        "\n",
        "2. **L2 Regularization [Ridge Regression]** - Adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
        "<figure>\n",
        "<img src='https://miro.medium.com/max/264/1*4MlW1d3xszVAGuXiJ1U6Fg.png' />\n",
        "</figure>\n",
        "* The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BhtD4xFwi3Y",
        "colab_type": "text"
      },
      "source": [
        "###Playing with Network Layer's Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVxBNgkMrIoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "8dcf9acb-1b89-42f4-ecfe-866fa0bb2bce"
      },
      "source": [
        "# in total = 25 layers in the model\n",
        "\"\"\"\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d_17 (Conv2D)           (None, 26, 26, 16)        160       \n",
        "_________________________________________________________________\n",
        "batch_normalization_15 (Batc (None, 26, 26, 16)        64        \n",
        "_________________________________________________________________\n",
        "dropout_15 (Dropout)         (None, 26, 26, 16)        0         \n",
        "\"\"\"\n",
        "#####################\n",
        "# Layers in the network\n",
        "print(\"Total Number of layers in the network = \", len(model.layers)) # 25 layers\n",
        "# print(len(model.layers[0].get_weights())) # 2 - Weights and Bias\n",
        "\n",
        "#####################\n",
        "## Weights ##\n",
        "# print(len(model.layers[0].get_weights()[0])) # 3 [kernel lenght]\n",
        "# print(len(model.layers[0].get_weights()[0][0])) # 3 [kernel breadth]\n",
        "# print(len(model.layers[0].get_weights()[0][0][0])) # 1\n",
        "# print(len(model.layers[0].get_weights()[0][0][0][0])) # 16 kernels used\n",
        "\n",
        "print(\"Printing 3X3X16 weights = \", (model.layers[0].get_weights()[0])) # print \n",
        "#####################\n",
        "## Bias ##\n",
        "print(len(model.layers[0].get_weights()[1]))\n",
        "print(\"Printing 16 Bias = \", (model.layers[0].get_weights()[1]))\n",
        "#####################"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of layers in the network =  25\n",
            "2\n",
            "Printing 3X3X16 weights =  [[[[-2.1873840e+01  1.8829962e+01 -6.5906892e+00 -1.2896430e+01\n",
            "     2.0089771e+01  2.4444301e+00  9.9514465e+00  7.4230903e-01\n",
            "    -1.3540105e+01  6.3781199e+00 -1.3767236e+01 -7.4635845e-01\n",
            "     3.5003061e+00  1.0436450e+01  7.8123007e+00 -4.2900329e+00]]\n",
            "\n",
            "  [[-1.9227097e+01 -1.6605350e+01 -1.2980994e+01 -3.5311306e+01\n",
            "     1.4055423e+01  5.4505258e+00 -1.2234921e+01  1.9503735e+01\n",
            "    -1.7080641e+01  1.3780191e+01  9.7480679e+00 -2.1409992e+01\n",
            "     1.4395055e+01  8.6534691e+00  9.1757879e+00 -5.0676451e+00]]\n",
            "\n",
            "  [[-5.2524800e+00 -2.7742958e+01 -9.3566113e+00  8.6971319e-01\n",
            "     1.3313248e+01  1.0907667e+01 -2.6824421e+01  9.7205162e+00\n",
            "     3.0619280e+00  3.7053032e+01  1.8988211e+01 -2.6007935e+01\n",
            "     1.5351596e+01 -2.2785675e+01 -8.7142820e+00 -6.5434661e+00]]]\n",
            "\n",
            "\n",
            " [[[ 2.8670868e+01  4.2611694e+01 -1.6140779e+01  4.6697664e+00\n",
            "     2.4490047e+00 -1.8967342e+01  1.9364622e+01 -2.1358385e+01\n",
            "    -1.6665045e+01 -1.0450955e+01 -9.3605614e+00  1.8523335e+01\n",
            "     2.4581009e+01  1.3191623e+01  1.1421599e+01 -7.6796494e+00]]\n",
            "\n",
            "  [[-1.9788103e+01 -1.8254568e+01 -1.3390898e+01  1.0697549e+01\n",
            "    -3.0132663e+00 -8.6462507e+00 -9.2245054e+00 -2.4471900e+00\n",
            "    -2.0012055e-02 -5.4550581e+00  9.4632425e+00 -1.9056936e+01\n",
            "     1.6353277e+01  7.0857626e-01 -1.1157386e+01 -1.1907435e+01]]\n",
            "\n",
            "  [[-2.7163498e+01 -1.1720084e+01 -8.3315220e+00  1.0474348e+01\n",
            "    -4.9168010e+00 -1.1584539e+01 -1.8740812e+01  1.6720291e+01\n",
            "     1.6226007e+01  2.1134947e+01  2.9445646e+01  7.5387645e+00\n",
            "    -5.1201421e-01 -2.6414804e+01 -2.2720922e+01 -9.3200808e+00]]]\n",
            "\n",
            "\n",
            " [[[ 8.9632540e+00  7.8735147e+00  1.8030762e+01  1.3115345e+01\n",
            "    -1.8798441e+01 -1.2523549e+01  7.7006426e+00 -1.5428735e+01\n",
            "    -7.8392525e+00 -1.6040255e+01 -1.0492639e+01  9.3892708e+00\n",
            "    -2.5744312e+00  1.5945784e+01 -2.7093410e+01 -6.0285511e+00]]\n",
            "\n",
            "  [[-4.9361105e+00  1.6491373e+01  8.4722958e+00  1.1266892e+01\n",
            "    -2.1145805e+01 -1.8226240e+01 -1.3641154e+01  8.1859417e+00\n",
            "    -9.0699081e+00 -1.7276529e+01 -2.3919615e+01  1.3470233e+01\n",
            "    -3.2754772e+01  1.5431962e+01 -1.4528638e+01 -7.6813350e+00]]\n",
            "\n",
            "  [[-1.4295417e+01  6.8524532e+00  9.5720005e+00  2.7429001e+00\n",
            "    -1.8340021e+01 -1.4923623e+01 -9.6371679e+00  1.6786167e+01\n",
            "     1.1191930e+01 -5.7302799e+00 -5.7196193e+00  1.9566771e+01\n",
            "    -2.0911215e+01 -1.6465979e+01  2.4179344e+01 -1.2640581e+01]]]]\n",
            "16\n",
            "Printing 16 Bias =  [-14.705782   -8.382469  -18.257889   29.617664   12.844184  -11.15303\n",
            " -10.390258   -6.669859   -4.9904933 -11.481095   -3.906238   17.599745\n",
            "  15.513684   14.093019   -3.125639    4.6505513]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfZmUWs8nn2",
        "colab_type": "text"
      },
      "source": [
        "### Defining Custom Loss with L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzIyoKx8mKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################################################################\n",
        "# Regularization Code\n",
        "import keras.backend as KBackend\n",
        "lambd = 0.01\n",
        "\n",
        "def reg_term(lambd):\n",
        "    w2=0\n",
        "    for i in range(len(model.layers)):\n",
        "        if len(model.layers[i].get_weights()) > 0:\n",
        "            t = np.sum(model.layers[i].get_weights()[0] * model.layers[i].get_weights()[0])\n",
        "            w2=w2+t\n",
        "    w2 = w2*(lambd/2*X_train.shape[0])\n",
        "    return w2\n",
        "\n",
        "def loss_l2(y_true, y_pred):\n",
        "    return KBackend.categorical_crossentropy(y_true, y_pred)+ reg_term(lambd)\n",
        "#########################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWUU8DtDFySF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##We tried L2 Regularization with:\n",
        "**1. Without Image Normalization**\n",
        "\n",
        "**2. With Image Normalization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozUBl5SrzvdA",
        "colab_type": "text"
      },
      "source": [
        "### Without Image Normalization\n",
        "1. **With Initial LR-0.01**: epochs=20, Validation Accuracy = 99.32%\n",
        "2. **With Initial LR-0.1**: epochs=20, Validation Accuracy = 99.48%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygx0wFXR_yCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Preparing the Un-Normalized Data for the model.\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjaMl4yD2i4c",
        "colab_type": "text"
      },
      "source": [
        "#### LR 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpIabQO82kEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cae24364-8783-4a0e-b885-93530b1abef9"
      },
      "source": [
        "model.compile(loss=loss_l2, optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit(X_train, Y_train, \n",
        "          batch_size=128, \n",
        "          epochs=20, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, Y_test), \n",
        "          callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################\n",
        "# [0.019800396383114274, 0.9942] LR - 0.01; epochs 20; [0.01, 0.0075815011, 0.00141623 ]\n",
        "# [22743352.0, 0.9932]           LR - 0.01; epochs 20; [0.01, 0.0075815011, 0.00141623 ]; loss: 22743352.0000 - acc: 0.9586 - val_loss: 22743352.0000 - val_acc: 0.9932"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 22743352.0213 - acc: 0.8931 - val_loss: 22743354.0000 - val_acc: 0.0982\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0075815011.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9432 - val_loss: 22743352.0000 - val_acc: 0.9254\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0061050061.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9485 - val_loss: 22743352.0000 - val_acc: 0.9893\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.005109862.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9503 - val_loss: 22743352.0000 - val_acc: 0.9892\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0043936731.\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 22743352.0000 - acc: 0.9526 - val_loss: 22743352.0000 - val_acc: 0.9902\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0038535645.\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 22743352.0000 - acc: 0.9521 - val_loss: 22743352.0000 - val_acc: 0.9891\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.003431709.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9537 - val_loss: 22743352.0000 - val_acc: 0.9911\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0030931024.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9551 - val_loss: 22743352.0000 - val_acc: 0.9932\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0028153153.\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 22743352.0000 - acc: 0.9561 - val_loss: 22743352.0000 - val_acc: 0.9919\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0025833118.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9558 - val_loss: 22743352.0000 - val_acc: 0.9921\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0023866348.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9567 - val_loss: 22743352.0000 - val_acc: 0.9910\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0022177866.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9562 - val_loss: 22743352.0000 - val_acc: 0.9936\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.002071251.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9575 - val_loss: 22743352.0000 - val_acc: 0.9924\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019428793.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9566 - val_loss: 22743352.0000 - val_acc: 0.9946\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0018294914.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9574 - val_loss: 22743352.0000 - val_acc: 0.9939\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017286085.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9575 - val_loss: 22743352.0000 - val_acc: 0.9940\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.00163827.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9580 - val_loss: 22743352.0000 - val_acc: 0.9949\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0015569049.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9586 - val_loss: 22743352.0000 - val_acc: 0.9935\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0014832394.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 22743352.0000 - acc: 0.9585 - val_loss: 22743352.0000 - val_acc: 0.9940\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.00141623.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 22743352.0000 - acc: 0.9586 - val_loss: 22743352.0000 - val_acc: 0.9932\n",
            "[22743352.0, 0.9932]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmCT_yfH2ad_",
        "colab_type": "text"
      },
      "source": [
        "####LR 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyJnaiwgzXk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6a1165f-a685-4514-80c1-e387b05c0c8b"
      },
      "source": [
        "model.compile(loss=loss_l2, optimizer=Adam(lr=0.1), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit(X_train, Y_train, \n",
        "          batch_size=128, \n",
        "          epochs=30, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, Y_test), \n",
        "          callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "#########################################################################\n",
        "# [0.019371943644070144, 0.9944] LR - 0.1; epochs 30; [0.1, 0.0758150114, 0.0097551458 ]; loss: 0.0823 -        acc: 0.9571 - val_loss: 0.0194 -        val_acc: 0.9944 \n",
        "# [24289262.2624,        0.9948] LR - 0.1; epochs 30; [0.1, 0.0758150114, 0.0097551458 ]; loss: 24289263.8848 - acc: 0.9585 - val_loss: 24289262.5632 - val_acc: 0.9948"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1.\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 24289264.0000 - acc: 0.8830 - val_loss: 24289263.6384 - val_acc: 0.9638\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0758150114.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 24289263.9915 - acc: 0.9349 - val_loss: 24289263.4592 - val_acc: 0.9783\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0610500611.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9915 - acc: 0.9416 - val_loss: 24289263.0752 - val_acc: 0.9861\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0510986203.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9573 - acc: 0.9462 - val_loss: 24289262.9472 - val_acc: 0.9882\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0439367311.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9787 - acc: 0.9463 - val_loss: 24289262.8960 - val_acc: 0.9895\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0385356455.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 24289263.9659 - acc: 0.9471 - val_loss: 24289263.0240 - val_acc: 0.9885\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0343170899.\n",
            "60000/60000 [==============================] - 12s 200us/step - loss: 24289263.9403 - acc: 0.9508 - val_loss: 24289262.9216 - val_acc: 0.9903\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0309310238.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 24289263.9531 - acc: 0.9497 - val_loss: 24289263.2032 - val_acc: 0.9847\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0281531532.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9275 - acc: 0.9511 - val_loss: 24289262.9216 - val_acc: 0.9922\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0258331181.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9275 - acc: 0.9520 - val_loss: 24289262.8704 - val_acc: 0.9922\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0238663484.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9232 - acc: 0.9521 - val_loss: 24289263.1008 - val_acc: 0.9902\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0221778665.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9403 - acc: 0.9539 - val_loss: 24289262.7168 - val_acc: 0.9933\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0207125104.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 24289263.9275 - acc: 0.9536 - val_loss: 24289262.8448 - val_acc: 0.9926\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0194287935.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9531 - acc: 0.9544 - val_loss: 24289262.7168 - val_acc: 0.9917\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.018294914.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.8933 - acc: 0.9552 - val_loss: 24289262.7680 - val_acc: 0.9924\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0172860847.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9403 - acc: 0.9538 - val_loss: 24289262.6912 - val_acc: 0.9934\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0163826999.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9317 - acc: 0.9552 - val_loss: 24289262.7424 - val_acc: 0.9937\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0155690487.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 24289263.9147 - acc: 0.9552 - val_loss: 24289262.7424 - val_acc: 0.9932\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0148323939.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9275 - acc: 0.9566 - val_loss: 24289262.7168 - val_acc: 0.9926\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0141623.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9104 - acc: 0.9550 - val_loss: 24289262.7168 - val_acc: 0.9933\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0135501355.\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 24289263.9147 - acc: 0.9555 - val_loss: 24289262.5888 - val_acc: 0.9942\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0129886998.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9189 - acc: 0.9538 - val_loss: 24289262.7680 - val_acc: 0.9931\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0124719381.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 24289263.9275 - acc: 0.9561 - val_loss: 24289262.6144 - val_acc: 0.9934\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0119947223.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 24289263.9104 - acc: 0.9548 - val_loss: 24289262.6656 - val_acc: 0.9938\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0115526802.\n",
            "60000/60000 [==============================] - 12s 199us/step - loss: 24289263.9275 - acc: 0.9561 - val_loss: 24289262.5632 - val_acc: 0.9938\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0111420613.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.9403 - acc: 0.9583 - val_loss: 24289262.4864 - val_acc: 0.9951\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0107596299.\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 24289263.8901 - acc: 0.9569 - val_loss: 24289262.6912 - val_acc: 0.9944\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0104025798.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.8805 - acc: 0.9570 - val_loss: 24289262.5888 - val_acc: 0.9948\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0100684656.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.8720 - acc: 0.9590 - val_loss: 24289262.6400 - val_acc: 0.9943\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0097551458.\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 24289263.8848 - acc: 0.9585 - val_loss: 24289262.5632 - val_acc: 0.9948\n",
            "[24289262.2624, 0.9948]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSL2Ziz30BH9",
        "colab_type": "text"
      },
      "source": [
        "### With Image Normalization\n",
        "1. **With Initial LR-0.1**: epochs=20, Validation Accuracy = 48.56%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4lKPM_Aakx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be81db8d-8990-400e-dec8-afb9d932cfc7"
      },
      "source": [
        "### Preparing Normalized date for the model.\n",
        "#########################################################################\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "#########################################################################\n",
        "# Image Normalization\n",
        "\n",
        "# Create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# Calculate the mean on the trainng dataset\n",
        "datagen.fit(X_train)\n",
        "print(\"Data Generator mean=%.3f, std=%.3f\" % (datagen.mean, datagen.std))\n",
        "\n",
        "# We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "#########################################################################"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYu-gxP73F8J",
        "colab_type": "text"
      },
      "source": [
        "####LR 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SBaJdcuodAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ed9a354-0163-45d1-9f5b-9748e9141b5c"
      },
      "source": [
        "#########################################################################\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "\n",
        "model.compile(loss=loss_l2, optimizer=Adam(lr=0.1), metrics=['accuracy'])\n",
        "model.load_weights('model.h5')\n",
        "model.fit_generator(iterator, \n",
        "                    samples_per_epoch=len(X_train), \n",
        "                    epochs=30,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test), \n",
        "                    callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "\n",
        "#########################################################################\n",
        "# [2.930216279411316, 0.7931] LR - 0.1;   epochs 30; [0.1, 0.0758150114  0.0097551458];       loss: 0.0790 - acc: 0.9597 - val_loss: 2.9302 - val_acc: 0.7931\n",
        "# [6729249.6336, 0.4856]      LR - 0.1;   epochs 30; [0.1, 0.0758150114  0.0097551458];       loss: 6729242.0000 - acc: 0.9542 - val_loss: 6729249.6336 - val_acc: 0.4856"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=30, verbose=1, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1000000015.\n",
            "468/468 [==============================] - 23s 50ms/step - loss: 6729242.1165 - acc: 0.8919 - val_loss: 6729249.2976 - val_acc: 0.5282\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0758150125.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0021 - acc: 0.9374 - val_loss: 6729252.6336 - val_acc: 0.3160\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0462851127.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0021 - acc: 0.9441 - val_loss: 6729249.0272 - val_acc: 0.5220\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0236510534.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9492 - val_loss: 6729249.5968 - val_acc: 0.4866\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0103915001.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9519 - val_loss: 6729249.7576 - val_acc: 0.4765\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0040044315.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9534 - val_loss: 6729249.3280 - val_acc: 0.5043\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0013742043.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9540 - val_loss: 6729249.5568 - val_acc: 0.4933\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0004250555.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9547 - val_loss: 6729249.5696 - val_acc: 0.4884\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001196665.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9564 - val_loss: 6729249.5824 - val_acc: 0.4893\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 3.09136e-05.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9535 - val_loss: 6729249.5696 - val_acc: 0.4896\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 7.3779e-06.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6729242.0000 - acc: 0.9542 - val_loss: 6729249.5824 - val_acc: 0.4892\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1.6363e-06.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9546 - val_loss: 6729249.5824 - val_acc: 0.4891\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 3.389e-07.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9535 - val_loss: 6729249.6272 - val_acc: 0.4864\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 6.58e-08.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9547 - val_loss: 6729249.5632 - val_acc: 0.4892\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1.2e-08.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9540 - val_loss: 6729249.5888 - val_acc: 0.4884\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 2.1e-09.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6729242.0000 - acc: 0.9541 - val_loss: 6729249.6080 - val_acc: 0.4873\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 3e-10.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9544 - val_loss: 6729249.5632 - val_acc: 0.4905\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9522 - val_loss: 6729249.5824 - val_acc: 0.4883\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9532 - val_loss: 6729249.5696 - val_acc: 0.4902\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9539 - val_loss: 6729249.6336 - val_acc: 0.4863\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0011 - acc: 0.9550 - val_loss: 6729249.6080 - val_acc: 0.4870\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9530 - val_loss: 6729249.6016 - val_acc: 0.4874\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9528 - val_loss: 6729249.5760 - val_acc: 0.4886\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9549 - val_loss: 6729249.5568 - val_acc: 0.4901\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9550 - val_loss: 6729249.5568 - val_acc: 0.4908\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9545 - val_loss: 6729249.5696 - val_acc: 0.4887\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6729242.0000 - acc: 0.9535 - val_loss: 6729249.5440 - val_acc: 0.4924\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6729242.0000 - acc: 0.9543 - val_loss: 6729249.5696 - val_acc: 0.4890\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6729242.0000 - acc: 0.9541 - val_loss: 6729249.5568 - val_acc: 0.4907\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6729242.0000 - acc: 0.9542 - val_loss: 6729249.6336 - val_acc: 0.4856\n",
            "[6729249.6336, 0.4856]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbSnBe8JjcKd",
        "colab_type": "text"
      },
      "source": [
        "# Relu After BN\n",
        "### Creating the Model by appling Relu Activation Function after the Batch Normalization Layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dzawOAGJiCR",
        "colab_type": "text"
      },
      "source": [
        "### Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzHCgYuK3goB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "698d3d3f-5e14-414a-df96-72d18b281a46"
      },
      "source": [
        "########################################\n",
        "model2 = Sequential()\n",
        "########################################\n",
        "\n",
        "# All shapes are in the form [height, width, num_channels]\n",
        "\n",
        "# INPUT SIZE : [28, 28, 1] | OUTPUT SIZE : [26, 26, 16] | GRF_of_input : [1, 1]\n",
        "model2.add(Convolution2D(16, 3, 3, input_shape=(28,28,1)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [26, 26, 16] | OUTPUT SIZE : [24, 24, 32] | GRF_of_input : [3, 3]\n",
        "model2.add(Convolution2D(32, 3, 3))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [24, 24, 32] | OUTPUT SIZE : [24, 24, 10] | GRF_of_input : [5, 5]\n",
        "model2.add(Convolution2D(10, 1, 1, activation='relu'))\n",
        "\n",
        "########################################\n",
        "# INPUT SIZE : [24, 24, 10] | OUTPUT SIZE : [12, 12, 10] | GRF_of_input : [5, 5]\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [12, 12, 10] | OUTPUT SIZE : [10, 10, 16] | GRF_of_input : [10, 10]\n",
        "model2.add(Convolution2D(16, 3, 3))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [10, 10, 16] | OUTPUT SIZE : [8, 8, 16] | GRF_of_input : [12, 12]\n",
        "model2.add(Convolution2D(16, 3, 3))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [8, 8, 16] | OUTPUT SIZE : [6, 6, 16] | GRF_of_input : [14, 14]\n",
        "model2.add(Convolution2D(16, 3, 3))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [6, 6, 16] | OUTPUT SIZE : [4, 4, 16] | GRF_of_input : [16, 16]\n",
        "model2.add(Convolution2D(16, 3, 3))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "# INPUT SIZE : [4, 4, 16] | OUTPUT SIZE : [1, 1, 10] | GRF_of_input : [18, 18]\n",
        "model2.add(Convolution2D(10, 4, 4))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "########################################\n",
        "\n",
        "# INPUT SIZE : [1, 1, 10] | OUTPUT SIZE : [10] | GRF_of_input : [22, 22]\n",
        "model2.add(Flatten())\n",
        "\n",
        "# INPUT SIZE : [10] | OUTPUT SIZE : [10] | GRF_of_input : [22, 22]\n",
        "model2.add(Activation('softmax'))\n",
        "########################################\n",
        "# Printing out the model summary\n",
        "model2.summary()\n",
        "\n",
        "# saving the model to reload model with initial weights while trying out different optimizations\n",
        "model2.save_weights('model_ReluAfterBN.h5')\n",
        "########################################"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 16,604\n",
            "Trainable params: 16,360\n",
            "Non-trainable params: 244\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7871EH6hCvfT",
        "colab_type": "text"
      },
      "source": [
        "## Training and Testing\n",
        "* ### Using 40 epochs\n",
        "* ### Saving Best Validation Accuracy Model using Checkpoint in Keras\n",
        "* ### Model File Name = Model_bestValAccuracy.hdf5\n",
        "\n",
        "### Results:\n",
        "1. LR=0.1, Epochs = 40, Validation Accuracy = 72.08%\n",
        "2. LR=0.1, Epoch = 3, Validation Accuracy = 78.84% \n",
        ">**[Epoch 3 has best validation accuracy in 30 epochs]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHBaUSmbMmaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ff9e981-ad28-4e75-95fc-a13514894219"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "#########################################################################\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "#########################################################################\n",
        "# Image Normalization\n",
        "\n",
        "# Create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# Calculate the mean on the trainng dataset\n",
        "datagen.fit(X_train)\n",
        "print(\"Data Generator mean=%.3f, std=%.3f\" % (datagen.mean, datagen.std))\n",
        "\n",
        "# Demonstrate effect on a single batch of samples\n",
        "# We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.\n",
        "iterator = datagen.flow(X_train, Y_train, batch_size=128)\n",
        "\n",
        "#########################################################################\n",
        "\n",
        "model2.compile(loss=loss_l2, optimizer=Adam(lr=0.1), metrics=['accuracy'])\n",
        "model2.load_weights('model_ReluAfterBN.h5')\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"Model_bestValAccuracy.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "model2.fit_generator(iterator, \n",
        "                    samples_per_epoch=len(X_train), \n",
        "                    epochs=40,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test), \n",
        "                    callbacks=[LearningRateScheduler(scheduler, verbose=1), checkpoint])\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "\n",
        "#########################################################################\n",
        "# [6472157.252, 0.7208]  LR - 0.1; epochs 30; [0.1, 0.0758150114  0.0097551458]; loss: 6472153.0064 - acc: 0.9503 - val_loss: 6472157.2792 - val_acc: 0.7208 # after 40 epochs\n",
        "# [6472156.0576, 0.7874] LR - 0.1; epochs 3; [0.1, 0.0758150114  0.0097551458]; loss: 6472153.0064 - acc: 0.9415 - val_loss: 6472156.0576 - val_acc: 0.7874 # best Model\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generator mean=33.318, std=78.567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=40, verbose=1, validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.1000000015.\n",
            "468/468 [==============================] - 24s 52ms/step - loss: 6472153.1549 - acc: 0.8810 - val_loss: 6472159.8880 - val_acc: 0.5577\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.55770, saving model to Model_bestValAccuracy.hdf5\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0758150125.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0203 - acc: 0.9315 - val_loss: 6472156.3264 - val_acc: 0.7776\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.55770 to 0.77760, saving model to Model_bestValAccuracy.hdf5\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0462851127.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0064 - acc: 0.9415 - val_loss: 6472156.0576 - val_acc: 0.7874\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.77760 to 0.78740, saving model to Model_bestValAccuracy.hdf5\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0236510534.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0075 - acc: 0.9446 - val_loss: 6472156.5936 - val_acc: 0.7528\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.78740\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0103915001.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0075 - acc: 0.9487 - val_loss: 6472156.2864 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.78740\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0040044315.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0053 - acc: 0.9484 - val_loss: 6472157.3424 - val_acc: 0.7156\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.78740\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0013742043.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9493 - val_loss: 6472157.3504 - val_acc: 0.7155\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.78740\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0004250555.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0075 - acc: 0.9515 - val_loss: 6472157.2856 - val_acc: 0.7189\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.78740\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001196665.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9516 - val_loss: 6472157.2728 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.78740\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 3.09136e-05.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0011 - acc: 0.9502 - val_loss: 6472157.2536 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.78740\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 7.3779e-06.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9498 - val_loss: 6472157.2400 - val_acc: 0.7219\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.78740\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 1.6363e-06.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9506 - val_loss: 6472157.2600 - val_acc: 0.7219\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.78740\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 3.389e-07.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0064 - acc: 0.9526 - val_loss: 6472157.2664 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.78740\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 6.58e-08.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0021 - acc: 0.9518 - val_loss: 6472157.2856 - val_acc: 0.7201\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.78740\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 1.2e-08.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0043 - acc: 0.9514 - val_loss: 6472157.2656 - val_acc: 0.7220\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.78740\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 2.1e-09.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0021 - acc: 0.9514 - val_loss: 6472157.2600 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.78740\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 3e-10.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0032 - acc: 0.9511 - val_loss: 6472157.2792 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.78740\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0032 - acc: 0.9509 - val_loss: 6472157.2472 - val_acc: 0.7221\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.78740\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0096 - acc: 0.9505 - val_loss: 6472157.2728 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.78740\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9515 - val_loss: 6472157.2600 - val_acc: 0.7218\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.78740\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9495 - val_loss: 6472157.2656 - val_acc: 0.7217\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.78740\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9499 - val_loss: 6472157.2728 - val_acc: 0.7213\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.78740\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0021 - acc: 0.9528 - val_loss: 6472157.2848 - val_acc: 0.7212\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.78740\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0053 - acc: 0.9519 - val_loss: 6472157.2792 - val_acc: 0.7212\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.78740\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0053 - acc: 0.9501 - val_loss: 6472157.2592 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.78740\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0032 - acc: 0.9514 - val_loss: 6472157.2592 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.78740\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0032 - acc: 0.9508 - val_loss: 6472157.2984 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.78740\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0064 - acc: 0.9505 - val_loss: 6472157.2856 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.78740\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0064 - acc: 0.9508 - val_loss: 6472157.2528 - val_acc: 0.7220\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.78740\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0053 - acc: 0.9518 - val_loss: 6472157.2656 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.78740\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0032 - acc: 0.9496 - val_loss: 6472157.2472 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.78740\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0011 - acc: 0.9509 - val_loss: 6472157.2720 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.78740\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0043 - acc: 0.9507 - val_loss: 6472157.2656 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.78740\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 30ms/step - loss: 6472153.0064 - acc: 0.9519 - val_loss: 6472157.2784 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.78740\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0064 - acc: 0.9503 - val_loss: 6472157.2792 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.78740\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0053 - acc: 0.9502 - val_loss: 6472157.2728 - val_acc: 0.7206\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.78740\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0053 - acc: 0.9517 - val_loss: 6472157.2728 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.78740\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0053 - acc: 0.9505 - val_loss: 6472157.2728 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.78740\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0043 - acc: 0.9496 - val_loss: 6472157.2472 - val_acc: 0.7217\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.78740\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0.\n",
            "468/468 [==============================] - 14s 29ms/step - loss: 6472153.0064 - acc: 0.9503 - val_loss: 6472157.2792 - val_acc: 0.7208\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.78740\n",
            "[6472157.252, 0.7208]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DDsr6Nd7Um4",
        "colab_type": "text"
      },
      "source": [
        "# Find out 25 misclassified images from the validation dataset and create an image gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POButtmYLv5G",
        "colab_type": "text"
      },
      "source": [
        "### Creating array of first 25 misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlvH2xMwQzNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f4e8556-19d0-41bc-d419-e266980e2f02"
      },
      "source": [
        "count = 0\n",
        "misclassified_images = [] # index, original, predicted\n",
        "model2.load_weights('Model_bestValAccuracy.hdf5')\n",
        "for idx, single_prediction in enumerate(model2.predict(X_test)):\n",
        "    # print(np.argmax(single_prediction), y_test[idx])\n",
        "    predicted_class = np.argmax(single_prediction)\n",
        "    if(predicted_class != y_test[idx]):\n",
        "      count+=1\n",
        "      misclassified_images.append((idx, y_test[idx], predicted_class))\n",
        "      if(count==25):\n",
        "        break\n",
        "\n",
        "print(misclassified_images)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(7, 9, 4), (8, 5, 4), (9, 9, 7), (18, 3, 2), (20, 9, 7), (29, 1, 2), (31, 1, 4), (37, 1, 2), (46, 1, 2), (61, 8, 5), (64, 7, 2), (65, 4, 2), (73, 9, 3), (78, 9, 3), (84, 8, 2), (92, 9, 3), (94, 1, 2), (96, 1, 2), (97, 7, 2), (110, 8, 2), (111, 7, 2), (114, 7, 2), (115, 4, 9), (118, 9, 7), (128, 8, 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVvgl2ecLm8O",
        "colab_type": "text"
      },
      "source": [
        "### Plotting Image Gallery of Misclassified Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTb9cOlTXP5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "21f0fcdd-cccd-47ca-fe07-bfdfeb31219e"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "fig, ax = plt.subplots(5, 5)\n",
        "fig.tight_layout()\n",
        "for index, (img_idx, original, prediction) in enumerate(misclassified_images):\n",
        "  plt.subplot(5, 5, index+1)\n",
        "  plottable_image = np.reshape(X_test[img_idx], (28, 28))\n",
        "  plt.text(0, 0, 'pred=' + str(prediction))\n",
        "\n",
        "  plt.imshow(plottable_image, cmap='binary')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEcCAYAAAAV2MmlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXl41VT6gN+vlNIC1aJlKVtbtWwC\ngiCLIi6DI8NUAUUHcUEdBBdcQHRAUXGZURF3ZxBQRBQFVFTgh6A4zIjCDBQEoZRtoOzIIksLWGj7\n/f646bVAl9s2ubm5Pe/znIckNzn57stJv5vk5ERUFYPBYDAYnCDC7QAMBoPBEL6YJGMwGAwGxzBJ\nxmAwGAyOYZKMwWAwGBzDJBmDwWAwOIZJMgaDwWBwDM8nGRHJLsc2b5Rnu8qA8Wk/ZXEjIgtFZIVV\ndorIF07G5kXK6HOKiKwTkdUiMlFEqjoZmxdxun2GZJIRkSoO1t0eqOVU/aGI8Wk/TjlV1UtVtY2q\ntgEWAzOc2E+o4WAbnQI0A1oBMcAAh/YTUoRS+wx6khGRJBFZa/3CyBCRT0WkuohkisiLIrIcuEFE\nzhWRuSKyzMqezaztk0VksYisEpHnyrjvKsBLwKMOfDVXMD7tx02nhWI4A7gS8PyZjJs+VXWOWgBL\ngIYOfMWg4rn2qapBLUASoMAl1vxEYBiQCTxaaL1vgRRruiPwT2t6JnCbNX0fkG1NxwIriiktrHUe\nBIZY09nB/u7GpzeKm04L1X0b8KnbLsLIZ1VgOXCp2z7CxGfA7bOiX7Y7sA7YCAwvg6CtheYLsmEm\nkGgtqwkcO+VLZlif7QeqWtNnEOAfN6A+8D0Qac2H5B/Fsjo1PsOnjZ4Sw1fA9W77CyOfE4DX3PYX\nRj4Dbp+RlBPrUsnfgauA7cBSEZmpqmsC2PzUAdMK5o9Y/0YAB9V33S+Q7RGRWGBhMev3A5KB84CN\nIgJQXUQ2qup5AcQbFCrg1PgsAq+10YK4RCQe6AD0DiDOoOFhn08BtYFBAcQZNDzss2ztswIZuDMw\nr9D8CGBEgFlYgc7W/DvAw/iycHyh9RYBN1jTAlxgTc8EbrGm76Gcv6DLu52TpTxOjc/wa6PA3cD7\nbvsLB5/4bvQvAmLc9hcOPsvTPsXaqMyISB+gu6oOsOZvBTqq6uDitomPj9eEhAQ2bNhAjRo1OHr0\nKNHR0SQnJ5Oenk7z5s2JjPSdXOXk5LB161ZOnDiBqlKrVi3q169PTk4OmzdvJi8vj7i4OPbs2UPb\ntm0BWLZs2T5VrR1g/NmqWrNcX94hyurU+CyZ8vhMSkoiJyfHEaeB+hSRfwEvqOpc22TYQKgd85mZ\nmezbt09KiTkX2AJkWYtmqOozdvioKKHmEwI75svaPst9uSxQRGQgMBCgcePGzJo1i9TUVFavXu3E\nvrYEum6o/UEMFOPTXk71mZaWRmZmpiNOA/WpqpfbuuMgE6w22r59+1LXUVXH/8Y5Tagd82VtnxXp\nwrwDaFRovqG17NSAxqtqe1VtX7t2QD+KKzOlOjU+y4TxaS/mmLeXSuGzIll+KZAiIsn4xPTFd0O4\nRJKSkhzJwGFCmZ2W1+fKlSv904mJibz66qu0bNmSG264ocx1hTCmjdqLLT6zs7PZtm0bY8eOBeCN\nN95wJtrQp1K0z3InGVXNFZHBwDygCjBRVdNti6wSEgyns2fPZtasWUyYMMG/rGnTpmRmZpKTkxNW\nSca0UXuxw2d2djYvvfQSzz77rH/ZgQMHeP311znrrLPsDTjEqTTtM5i9Kdq1a6elcejQIb3nnnu0\na9euevz48VLXLwyQFszv43YJxKeq6saNG3XIkCFao0YNjYiIUBEpthifzlHZfGoRTkeMGFFku0tI\nSNB58+aVyadVt+vf0U2fgZCZmalDhw7VVatWlbquE2005G6KtW7dmq1btwJw+PBhzj77bJcj8j7b\nt2/ntddeK/KzZs2a0bJlyyBH5H02btzIvn37+Pzzz/nXv/7Fvffey8UXX0xKSorboYU0ycnJAIgI\ngwcP5vzzz2f48OHs3r2bnj178pe//IVHH32U6tWruxyp91m/fj1vvfUWkydP5tChQ0ybNo3Zs2ez\nbds2EhMTad26dXACCaUsvG3bNgX8v25uuukm3b9/f6nZ18ksHMqlJJ979+7Vxx9/XFVVFy1apHFx\ncdqwYUONi4vTP/3pT/r111/r3r17NTs72/gMsH2qqv700086aNAgrV27tuJ7VsFfqlatqi1bttS7\n775bc3JyKr3Popz+4Q9/UBHRvn37+pd99913Gh8f7z/ub7nlloCuYpgzmaLJy8vTVatWab169U5r\no2eccYYC2rlzZ83LyzttWyfaaEgJevDBB09KMiKicXFxOmbMmFLFOiUolEtxPrOzs7VNmzYnXf7a\nvHmzqqpu2bKlyMZlfJbePleuXKlnnnmm/4Bt2LCh3nTTTTpixAjt2LGjApqQkKCNGzfWsWPHVnqf\nRTktOL5PvXTzww8/aNeuXf3H/c0331zi/4WqSTLF8cQTT5yUWOLi4jQiIuKkZbVq1dITJ06ctm1Y\nJ5kvvvhCfSdWqseOHdN3331XzzvvPL+UQM5oKttBfKrPrKws/zXvOnXq6MGDB0t1VhKV3aeqry0+\n/fTTKiIK6FNPPVXk2d+qVat03rx5Jx3Ie/bsqdQ+i3J6//33q4j4f/RMmDBBp06d6i+DBw8u8v5g\nUZgkczJ5eXnau3dvjYiI0DZt2mhaWpo+//zz2qxZs5Pa5dVXX11sHWGdZKZNm3Zawyo4tQY0Kyur\n2G2dFBTK5VSfH3zwgYqIJiUl6bZt20r1VRqV3aeq6ldffaU1a9ZUQBs0aHDa57m5ubp582Zt3Lix\nxsbG+g/kW2+99bRLPpXNZ1FO33//fRUR/fLLLzUjI0OrVatWbCeUTz75pMQfSibJnMyrr76qgDZr\n1kynTp2q0dHRp10ua9KkiWZmZhZbhxNtNGRu/H/88cenLUtLS/NP16zpyQfKg8qiRYsAaNu2LQ0b\nev61GSFBbm4uVar43v9UtWpVHn30UdauXQtATEwMGRkZZGRkkJub69+mbt26jBw5kqpVzUsYT6VX\nr15MnjyZK6+8kp9//pno6GiOHz9e5Lo33ngj1atXZ8KECfTs2dN0BiiBEydO8OKLLwKwdu1a+vbt\nC8BZZ53F/fffz/z58/nhhx+48847SUxMDG5woZKFC85kMjIydPr06XrTTTdpZGSk1qpVSwFNT08v\ndlsns3Aol1N91q5dW0VEo6OjddSoUbp8+fJSnZVEZfepqnr06FHt1auX1qhRw39WDWhkZORpvxIj\nIiK0T58+unPnTuOzBKeFmTVrlqamppbYrV5EtHXr1qfdxzFnMr+Rm5urnTt3VkBjYmI0Li5OR44c\nqb/88ouuXLlSo6OjtVOnTnro0KES/z+caKMhIUhVdf/+/RoXF3fSjf/f//73umHDBm3atKkOGjSo\nRDlOCQrlUtxN1YISGRmpH3zwgf7tb3/T6dOna3p6uqanp+v06dNLdWl8nsyBAwf0L3/5i/bs2VPv\nv/9+HThwoP9mf0G555579MCBA8ZngE4LyM3N1QMHDuiBAwd07dq1um7dOj1w4IA+8MADWrNmzZP+\nHvz444/+7UySOZkDBw7oBx98oBkZGf5lWVlZ2rt3bwV03bp1pf1XhHeSUVX95ptv/AfsAw88oMeO\nHVNV3wNcSUlJunHjxqALCuVyqs9hw4aV+ouwoPzpT38q0aXxWTq33nqrv72+8847mpubW+L6lc2n\nBuB07969JX7+ww8/aOvWrf3ttnv37v7PTJIpnUmTJvnbaCCEfZIpCRHRRo0aldjLrLIdxKX5nD9/\nvnbs2LHYRNOrV68Stzc+i2bChAkn3eRv2bJlQNtVNp9aitMpU6ZodHS0RkRE6KJFi0p0t2TJEo2M\njDypc5BJMiVTcPms4EdQIDjRRisyCnNQ6du3L9u3b+ett95yOxTP8Lvf/Y5evXoBvpvWAwcOZOnS\npfTrV+oYfIZiWLJkCQ8//DBZWb7Xk8TGxvoHejSUjX79+vHss8+iquTl5ZW47k8//UR+fn6QIvM+\nmzZt8g+imZqayu233+5eMKGahU/lxx9/1JiYGBWRYq8tUsl+KQbic9myZSedvVx55ZX+8cvuu+++\nErc1Pk9n5MiR/l+HNWrU0AULFpS6TQGVzacG4PTQoUMqIpqcnKx33HHHacf2a6+9pi1bttTo6OjT\nnp8xZzJFs337dk1KSlJAGzdurFu2bCl1mwKcaKOlr+B738ECYA2QDjxoLR+Fb3jqFVbpYYegkhgz\nZowCet111+nRo0eDIsjuEmyfR48e1b59+xZ5qaykIWWMz6KJioryJ5lAOqN4zacbTpOTk/1JJDIy\nUqOiovzl1HbbsWNH/3ZeSTLB9jlz5kx/Gx01alSp6xfGrSSTAFxoTccC64EWlqBhZdlZRZPMnj17\nNCUlRQFduXJlUATZXdzwuXv3bv3jH/+o9erV8/9qDATj82SysrL8B+8FF1zg75gSKF7wGWynBUya\nNEm7d++uDRo0KPL+YZcuXfTZZ5/V3bt3+7fxUJIJqs+CXrrVqlXTJUuWBOS/ACfaaKkPY6rqLmCX\nNZ0lIhlAg9K2c4LatWszf/58EhMTeeGFF/joo4/cCKNCuOGzbt26zJ49mw8++IDFixczatQoJ3cX\nVILp85///Kd/+pVXXiE6OtqJ3biOG220f//+9O/fn927d5OVlcWECROoWbMmTZo0oV27djRu3Jhq\n1ao5GYJjBNPnkSNHOHjwIAC1atUKjYfYy5iRk4CtwBn4snAm8BMwEahlRxYOhEOHDmlSUpJ++eWX\njmdhJ0uo+CwO4/NkGjZsqHXq1Cn2YcvS8JpPDfE26pUzmcLFaZ+vv/66Pv300yHVRssipyawDLjO\nmq+L721uEcBf8b3VrajtBgJpQFrjxo3L9cXdFORUMT695/OVV17RN998s5w2veVTPdBGvZZkQt2n\nqjNtNFA5VfG9InRoMZ8nAatLq8f88jY+nSjGZ+V06qUk4wWfqs60UbG+YLGIiADvA7+o6kOFlieo\n71ojIjIE6KiqfUupKwtYV+IOy048sM+aTlTV2jbXbys2+9wLHOG3728XBU6NT3vwjE/w1DFvfNqD\no39DA0kyXYCFwCqg4Gmox4CbgDb4ettkAoMKhJVQV5qqtq9gzI7X6SR2+rTqq9ROjU/7Mce8vVR2\nn4H0LvsekCI+mmN/OOGP8Wkvxqf9GKf2Utl9emZYGYPBYDB4j2AnmfEeqdNLGKf2Ynzai/FpL57z\nWeo9GYPBYDAYyou5XGYwGAwGxwhakhGR7iKyTkQ2isjwctbRSEQWiMgaEUkXkQet5aNEZIeIrLBK\nD3ujDz2MT3sxPu3HOLUXz/q0+8Gbogq+p1r/B5wDRAErgRblqMe2gea8XIxP4zPUi3FqfBaUYJ3J\ndAA2quomVT0OTAV6lrUSVd2lqsut6SzAtcE6Xcb4tBfj036MU3vxrM9gJZkGwLZC89up4BcTkSSg\nLfBfa9FgEflJRCaKSK2K1O0BjE97MT7txzi1F8/69OSNfxGpCXwGPKSqh4GxwLn4np7dBbzsYnie\nw/i0F+PTfoxTewmmz2AlmR343g5XQENrWZkRkar45ExR1RkAqvqzquapaj4wAd+pZThjfNqL8Wk/\nxqm9eNZnsJLMUiBFRJJFJAroC8wsayXWQHPvAhmq+kqh5QmFVusNrK5gvKGO8Wkvxqf9GKf24lmf\npY5dZgeqmisig/ENdV0F33sT0stR1SXArcAqEVlhLXsMuElEThporuJRhy7Gp70Yn/ZjnNqLl32a\nJ/4NBoPB4BievPFvMBgMBm9gkozBYDAYHMMkGYPBYDA4hkkyBoPBYHAMk2QMBoPB4BgmyRgMBoPB\nMUySMRgMBoNjmCRjMBgMBscwScZgMBgMjuH5JCMi2WVYd4r1ZrnV1nDWVZ2MzYuUxWehbd4oz3aV\nBdNG7cX4tJcy+pwkIpsLvUGzTWnbhGSSEZEqDlU9BWgGtAJigAEO7SekcNAnItIeCPd3eZyGaaP2\nYnzai5PHPPCIqraxyorSVg56khGRJBFZa/3CyBCRT0WkuohkisiLIrIcuEFEzhWRuSKyTEQWikgz\na/tkEVksIqtE5Lmy7FtV56gFsATfcNmexk2fVkN+CXjUga/mGqaN2ovxaS9u+iwXTrzTuaQCJOEb\n6fMSa34iMAzfyJ+PFlrvWyDFmu4I/NOangncZk3fB2Trb++rXlFMaXFKDFWB5cClwf7+4eQTeBAY\nYk1nu+0iHJyaNmp8hrJPYBKwDvgJeBWoVmq8Ffyy3a0dbgSGl0HQ1kLzVwJfWIISrWU1gWOnfMkM\n67P9QFVr+gzK8ccN30t5XnO7sdjh1C2fQH3geyDSmg/JJGPaqPFpfNrnE0gABKgGvA88Wdo25X6f\njHWp5O/AVfjeN71URGaq6poANj/1/QIF80esfyOAg6pa3E2l095PICKxwMJi1u9XEJeIPAXUJgTf\nP1EBp0H3CSQD5wEbRQSguohsVNXzSok1aJg2ai/Gp7140aeq7rLmc0TkPXxnUKVEWv4M3BmYV2h+\nBDAiwCysQGdr/h3gYXxZOL7QeouAG6xpAS6wpmcCt1jT91C2LDzAqjfG7V8wdjl10+cpcYTcmYxp\no8an8Wm7z4RC9b0GvFDaNuV+aZmI9AG6q+oAa/5WoKOqDj5lvYHAQIAaNWq0S05OZsOGDdSoUYOj\nR48SHR1NcnIy6enpNG/enMhI38lVTk4OW7du5cSJE6gqtWrVon79+uTk5LB582by8vKIi4tjz549\ntG3bFoBly5btU9XaJcScC2wBsqxFM1T1mXIJcIBAnIaSz1Niz1bVmrYKqSDl8dmsWTNycnIccRqI\nz1Buo6F2zGdmZrJv3z4pJWbj096/of/Ed1Yo+C7B3a2qJXaBdvz1y6o6HhgP0L59e/30009JTU1l\n9Wr7X8ktIltKiSUor5t2klDyeUpcIZVgAuVUn2lpaWRmZjriNBCfpo0GTvv27QOJxfgsAwH8Db2y\nrHVWpAvzDqBRofmG1rIK0aRJEyIiIjhy5EjpK4cfjjitxBif9uKIz4iICG666SaOHTtW0aq8RqVo\nnxVJMkuBFKvPdRTQF9+1vhJJSkoqMQOLCCLC559/XoHQPEuZnZbmE+Ciiy7isssuY+vWrfZF6g1s\nb6NZWVk0adKECy64gO+++87eaEMf233u2rULEWH69OksXrzY3mhDH0f+hhawevVqVq5cycqVKyse\naQUod5JR1VxgMDAPyACmq2p6RQOqVcv38Hj//v0rWpXncMopwMKFC5kwYYIdVXkGJ3xWq1aNhg0b\nsnr1av7617/aEaZncMJnQkKCf/r5558nKyurhLXDC6eO92PHjvHWW2/Rrl072rZtS9u2bWnTpo1/\n+t5772XBggXBcx3M3hTt2rXT0vjiiy/0wgsv1KpVq+quXbtKXb8wQFowv4/bJRCfqqrjxo1TQO++\n++6A1i/A+CyaG2+8USMiIrRZs2aak5MT0Daqlc+nBug0IiLCX6ZMmRKgTVWrbte/YzBLID6vvvpq\nBVRE/KXwfMF08+bNdefOnSdt60QbDbmxy3r27MmTTz6JiPDMMyHRCSQssJ5lMdjIunXrKuMlHtup\nWvW3MSvT0tJcjMT7/Pe//2XevHkAdOzYkaeeeoqnnnqKhQsXMn36dL7++mvuvvtuEhMTycjIYPjw\n4Y7HFHJJBmDr1q3k5uYyZcoUt0MJCy699FJUlXHjxrkdisFwGh07dvRPT506lXXr1rkYjbf57LPP\nEBE6derE7Nmz/UmmS5cu9OnTh27dujF27Fi6deuGiLB8+XLHYwrJJHPgwAG3QwgrmjdvzqBBvoed\nL7roIpej8T733HOPf3rs2LEuRhIefPXVV5x11lkA/Pzzz1x11VUcPnzY5ai8SXJyMgB33HEHZ599\n9mmfb9q0ifr16/Puu+8C0K5dO8djCskks2nTJrdDCEvMJTN7iIyM9D/wtmbNmsra3d42qlevztCh\nQ/3z27dv59///reLEXmXe+65B1Vl1KhRrF279qTPBg0axLnnnsuuXbtQVXr06MHzzz/veEwhmWS+\n/vprwNdd1FzisYcuXbqgqmRnZ3P06FG3w/E0Xbp0oUmTJgCkp6cbnzbw8MMPc/HFF/vnn3vO+RHo\nw5VOnTqxa9cu/7NHmzZtYu7cuUyYMAERITExkWHDhvHxxx+f1LvPKUIyyXz55ZdcddVVqCrz5893\nO5yw4LrrrqNFixasW7futF84BoPbREVFERUV5e+RtHTpUrdD8ixz5syhZcuWrFy5kgsvvJALL7yQ\nP/zhD8THx/Pkk0/y448/8tJLLxEbGxuUeEJyyIWChwe//vpr03vHJqpXr05MTAyqynfffceFF17o\ndkiepuCPocE+EhMT/Zd0zaXd8vPJJ5+wf/9+AH8nigYNGpCRkRG0xFKYkDyTMThD8+bNERHTe8cG\nCkamMH8M7aNXr14nzefm5roUiTf5+uuvufHGGxk0aJD/vktBuf/++11JMGCSTKWiS5cuAGRkZLgc\nifdp2bKl2yGEHddeey2vvPKKf75bt24uRhP69OvXj3r16hEREUFERAQ9e/YkOjqaw4cP8+uvv/LB\nBx8QExODiATleZjiCNkk88EHHwC+4aq3b9/ucjThQ8HlMkPFcGIEXAM0bdrUP71y5Uq2bAl4IPBK\nR5MmTdizZ49/PiUlhauuuooHHniAiy++mNtuu41ff/0V8F0uc4uQTTIFp3a//PKLuflvI+YSjz0U\nvhRhsI/u3bvToUMHAA4fPswbb7zhckShy6hRoxg+fDixsbGoKqtWreK2225j0qRJLF++3N8+69ev\n7+oPy5C88Q++67Omh4m9FDz5b6g4vXv3Zs2aNSZhO8Af//hH/7FvOv6UzN/+9jfuu+8+pk6dys6d\nO/nnP/950ufXX389999/P2eeeaZLEQaQZESkETAZqIvvlZ/jVfV1ERkF3AXstVZ9TFXnOBVouOCm\nz4Ib/+H0h9Etn8F4vsAt3D7mb775ZkaNGgX4ut57Had9NmjQgIcfftiucG0nkMtlucDDqtoC6ATc\nJyItrM9eVdU2VrG1sXXu3JlLLrmEmjVr0rp1azurdhtXfBbQq1evcDubcdVnmOKq00aNGvHqq69S\nvXp1rryyzC9iDEUqdRstNcmo6i5VXW5NZ+F774Hjd5Euv/xyFi5cyKFDh8LqmQ63fBYwY8YM8vPz\ng7U7x3HL57333kt+fj55eXnUrl3sK9E9idtttGrVqjzwwANkZWWFxbHvtk+3KdONfxFJAtoC/7UW\nDRaRn0RkoojUKmabgSKSJiJpe/fuLWqVSovxaS/Gp/0Yp/ZSGX0GnGREpCbwGfCQqh4GxgLnAm2A\nXcDLRW2nquNVtb2qtg+3X3wVwfi0F+PTfoxTe6msPiWQ6/MiUhWYDcxT1VeK+DwJmK2qJT6hJiJZ\ngN2Pm8cD+6zpRFUN+f8FG33uBY7w2/e3iwKnxqc9eMoneOaYNz7twdG/oYH0LhPgXSCjsBwRSVDV\nXdZsbyCQp9PWqWr7ckVafHxpdtfpJHb6VNXaTnx/Lzk1Pu3HHPP2Utl9BvKczCXArcAqEVlhLXsM\nuElE2uDrkpcJDHIkwvDD+LQX49N+jFN7qdQ+S00yqvo9UNSDFWHZ3c5pjE97MT7txzi1l8ruM9jD\nyoz3SJ1ewji1F+PTXoxPe/Gcz4Bu/BsMBoPBUB5CdoBMg8FgMHgfk2QMBoPB4BhBSzIi0l1E1onI\nRhEp1xt0RKSRiCwQkTUiki4iD1rLR4nIDhFZYZUe9kYfehif9mJ82o9xai+e9Vn4vRhOFaAK8D/g\nHCAKWAm0KEc9CcCF1nQssB5oAYwChgXju4RCMT6Nz1AvxqnxWVCCdSbTAdioqptU9TgwFehZ1kq0\nkg80Vwjj016MT/sxTu3Fsz6DlWQaANsKzW+ngl9MyjHQXBhhfNqL8Wk/xqm9eNanJ2/8SzkHmjMU\njfFpL8an/Rin9hJMn8FKMjuARoXmG1rLyoz4Bpr7DJiiqjMAVPVnVc1T1XxgAr5Ty3DG+LQX49N+\njFN78azPYCWZpUCKiCSLSBTQF5hZ1kpKGmiu0GqBDjTnZYxPezE+7cc4tRfP+gxkgMwKo6q5IjIY\nmIevl8REVU0vR1WVeqC5AoxPezE+7cc4tRcv+zTDyhgMBoPBMTx5499gMBgM3sAkGYPBYDA4hkky\nBoPBYHAMk2QMBoPB4BgmyRgMBoPBMUySMRgMBoNjmCRjMBgMBscwScZgMBgMjuH5JCMi2WVYd4r1\n0p/V1kijVZ2MzYsYn/ZTRqfvishKazTcT62BDA2FMD7txeljPiSTjIhUcajqKUAzoBUQAwxwaD8h\nhfFpPw46HaKqF6hqa2ArMNih/YQUxqe9hNIxH/QkIyJJIrLWyogZ1q+L6iKSKSIvishy4AYROVdE\n5orIMhFZKCLNrO2TRWSxiKwSkefKsm9VnaMWwBJ8I5l6GuPTflx2etiqQ/AdxJ4f98n4tBfPHfNO\nvG6zpAIk4fuPvsSanwgMwzco26OF1vsWSLGmOwL/tKZnArdZ0/cB2frbq0RXFFNanBJDVWA5cGmw\nv7/xGfrFbafAe8DPwAKguts+jM/QKm77tNYN+Jiv6JftDqwDNgLDyyBoa6H5K4EvLEGJ1rKawLFT\nvmSG9dl+oKo1fUaBoDLGPQF4ze3GYodT4zNs22gV4B/AHW47ND6NzyJiCPiYL/dQ/9Y1v78DV+F7\nFehSEZmpqmsC2PzUU9aC+SPWvxHAQVVtE+D2iEgssLCY9fsVxCUiTwG1CcGhwSvg1PgsAq+2UQBV\nzRORqcCj+H6Ju47xaS9e9VnWY74i92Q6ABtVdZOqHgemAj0D3LaxiHS2pvsB3xf+UH3XUTeLyA3g\nu54qIhdYH/+A74U9ADcX2iZLVdsUUwrkDACuBm5S3xvgQo3yOjU+i8ZTbdSq47yC+oBrgbVl/9qO\nYXzai6d8WvWU+Zgv9/tkRKQP0F1VB1jztwIdVbXY3hvx8fGakJDAhg0bqFGjBkePHiU6Oprk5GTS\n09Np3rw5kZG+k6ucnBy2bt08VNAyAAAgAElEQVTKiRMnUFVq1apF/fr1ycnJYfPmzeTl5REXF8ee\nPXto27YtAMuWLdunqrVLiDkX2AJkWYtmqOoz5RLgAGV1anyWTHl8JiUlkZOT44jTAHxG4PsleQYg\nwErgHusPhuuE2jGfmZnJvn37pIR4jc8QOOYdfzOmiAwEBgI0btyYWbNmkZqayurV9r8tVUS2lPS5\nqgblTaBOYnzay6k+09LSyMzMdMRpAD7z8b250NMEq422b9++xM+Nz3Lty/ZjviKXy3YAjQrNN7SW\nnRrUeFVtr6rta9cuNkEafJTq1PgsE8anvZhj3l4qhc+KJJmlQIrV5zoK3zW+maVtlJSU5EgGDhPK\n7NT4LBHTRu3F+LSXSuGz3ElGVXPxPT07D8gApqtqul2BVUaC5TQlJcXuKkMS00btxfi0l2D6PH78\nOAATJ07k97//Pddccw2TJ09m//79TuzuJCr0xL/6nv5soqrnqupf7QqqMhMMp76ONpUDJ3yuWrWK\ngQMH0qZNG6Kjo0lISEBEePTRR+2oPqQJRvv8+uuv6dChA8888wz5+aHYadE+nPR56NAhXn/9dbp0\n6ULdunUBeP311/nXv/7F7Nmz6d+/P++953xvbtdu3M6fP58nn3ySxYsXExsbyx133MGHH35It27d\naNq0qX+9c889l6ysLOrWrcuWLVto3bo1x48f56qrriIqKqpS/cEsD/v372f8+PE88MAD1KhRw+1w\nPM/atWvp378/P/74o3/Z7t27AZg9ezb33XcfiYmJboXneebOnctNN93EoUOHSEtLY9iwYVSvXt3t\nsDzF8uXLWbRoEVOmTGHFihVcdtllrFnje/Rm1KhR9OvXjxMnTlC1alXq1avnfEAaxKdb27VrpwU8\n88wzWq1aNRWRcpesrCwtDJAWzO/jdinsszjWr1+vgH711Vf+ZSkpKaVup2p8FkWrVq0U34Ns/nLW\nWWf5p5955hnjs4xOC/jll180NjZWRUS7du2qIqKHDx8udn2rbte/Y6j43L9/v37yySf+tpiUlKTT\npk3zf/7MM8/4P0tNTdUlS5acVocTbdS1M5knnniCK664gmXLlp322Y4dO046jSu4npiVlXXSeitW\nrKBLly7OBmowFGLVqlX+6UGDBvHQQw9xxhln0KBBAwDS080tivKQlpbGvffeS3Z2Nj169OCzzz4j\nJiaGf/zjH/zlL39xOzxPMGbMGJ5//nlSUlIYMGAAQ4cO9T8z89133/Hkk08SGxvLAw88wMiRI4mO\njg5KXK4+59ClS5dik8To0aNPmn/zzTd58MEHAXjooYd45ZVXHI8vHHj77bcB34NZBfTr18+tcDxP\nREQE+fn5xMbGctVVVzFp0qSTevqcf/75LkbnTWrWrMnx48cZNmwYS5Ys4dixY7z00ksA/Oc//3E5\nutBn9erVXHTRRfz666+0atWKn376yf/Zli1baNWqFVlZWWzfvt3/YyiYeOZhuieeeALw3aO57rrr\nXI7GO8yYMYPIyEhq1arldihhwWWXXcaCBQvIysqiT58+p33etWtXF6LyLgcPHuTo0aMMHjyY6667\njjvvvJP/+7//Y+/evQA8/vjjLkcY+ixbtoxff/0VgAkTJviXHzhwgL59+5KVlcV5553nSoKBEEsy\nK1eupEqV3961k5ycTI0aNZg9ezaHDx+mVq1aLF68mPj4eBej9BaZmZlUrVqVr776ihUrVgAwbdo0\nRo0a5W5gHuWbb75h6dKlDBo06KRfjAAdOnQwSaaMrF+/HvBdzhk3bhwnTpzwf5acnEyrVq3cCs0z\ndOvWjXPOOYdNmzZxzTXXsHDhQvLy8rjlllv48ccfadOmDT/88IN7AYbKTauff/5ZExMTT7qx37Vr\nV+3du7fGxcWpiGjv3r112bJlum3btiLroJLdWA3kpiqn3KQuKIFgfBbPxx9/rA0aNPD7jIuL0w8+\n+MD4LIfTguP9zDPP9E9HRESUeNNf1dz4L8yYMWM0KipKAY2JidHatWsroImJibpr164SPRbGiTYa\nMq9fzsvLO+2SzsKFC/niiy84dOgQAF988QXt27ena9eupKamsmVLicPsGCw6d+7M9OnTWbhwIQsX\nLvT3mTeUn0OHDrFv3z7//MyZM7nllltcjMi7TJ48mcmTJ/Pvf//bv2zo0KHExsa6GJW3ePjhh1m1\nahWffPIJquq/3Lhlyxa6d+/Oyy+/7F5woZCFCzhy5Ig+9dRTOmDAAK1Vq1apXZjfeecdx7NwKJdA\nfiXOmjVLT5w44Z/fvHmzxsXFlbqdqvFZHB988IFWq1bNfxYjIpqXl2d8VsCpqurNN9+sIqJDhgw5\nqc0WhzmTKZpatWopoPXq1dObb75Z4+LiNCYmRj///HPNz88vcVsn2mjInMkAVK9enSFDhrBv3z4O\nHjwIwO23305WVhb5+fmnlT//+c8uRxz6pKam+rsxAvz8889+t4ayExERwa233kpOTg4JCQkcOHCA\n/Px8IiJC6lDyHHfeeScfffQRKSkpjBw58qQ2aygbBw4coF69emzYsIEPP/yQAwcOcPToUaZMmUJE\nRESRj404ScgdGSNGjODLL78E4J133uHNN980T6obQoL3338f3489H7fffjtxcXEuRhQerF27lk8/\n/ZRq1aoxZswYzjrrLLdD8jy7d+/2P19YQMGjC4MHF/u6GkcIuSTz0UcfAdC/f3/+9Kc/mQRjMzt3\n7nQ7BE+yfv36k7rTXnTRRQwdOtTFiMKHV199lezsbGrXrs0111zjdjhhS8EZTK9evYK631KTjIg0\nEpEFIrJGRNJF5EFr+SgR2SEiK6zSo6LBbN68mcOHD9OmTRv+/ve/h2WCCabPovjXv/7lRLWuESyf\nd911Fzt2+F71cdFFF/F///d/YduVPtht9OuvvwZg5MiRdlQXcgTbZ7NmzQD4/vuT3shMWloaMTEx\ndOrUyY7dBEwgFz5zgYdVdbmIxALLROQb67NXVXWMXcEsWbKE+Ph43njjjXAeFC9oPisJjvv88MMP\nWbJkCQCxsbE89NBDePHlUWUgqG20oJdo79697aw2lAiqz7Fjx3LFFVfwwAMP0KFDB+rVq0d+fj7z\n5s0jNTWVyy67zM7dlUqpZzKquktVl1vTWfjee2D7o6Off/45d9xxB6+88kpYj0cWLJ8l0bhx42Du\nzlGc9vm///2Pe++91/9E9YABA8J+WB632uiGDRsYPXq0P6GHC8H2efnllzN16lQOHTpE586defHF\nF+nVqxcpKSn+YaaCSlm6ogFJwFbgDGAUkAn8BEwEapWn+93+/fs1KipKRUTPOuusErvXlQYe6yLq\nhE87MT59dO/eXQGNjo6uiE7P+dQgtdE+ffqoiGjHjh31m2++CdinF7swV8ZjPuAb/yJSE/gMeEhV\nDwNjgXOBNsAuoMinfURkoIikiUhawQNChVFV/1ASlWlMMqd8Vlac9PnVV1+hqhw7dsyZ4EOUYLXR\nTz75hPz8fP7zn//QrVs3+75AiFFpj/lAMhFQFd8rQocW83kSsLq0eryYhZ0oxqfxGerFC069dCbj\nBZ+qzrRRsb5gsYjv1ZPvA7+o6kOFlieo6i5regjQUVX7llJXFrCuxB2WnXigYHyPRFUN6TuyNvvc\nCxzht+9vFwVOjU978IxP8NQxb3zag6N/QwNJMl2AhcAqoOCF248BN+E7zVN81xUHFQgroa40VW1f\nwZgdr9NJ7PRp1VepnRqf9mOOeXup7D5L7cKsqt8DUsRHc+wPJ/wxPu3F+LQf49ReKrvPkHvi32Aw\nGAzhQ7CTzHiP1OkljFN7MT7txfi0F8/5LPWejMFgMBgM5cVcLjMYDAaDYwQtyYhIdxFZJyIbRWR4\nOetwdXDJUML4tBfj036MU3vxrE+7H7wpqgBVgP8B5wBRwEqgRTnqSQAutKZjgfVAC3zDMwwLxncJ\nhWJ8Gp+hXoxT47OgBOtMpgOwUVU3qepxYCrQs6yVaAgMLhkiGJ/2Ynzaj3FqL571Gawk0wDYVmh+\nOxX8YiKSBLQF/mstGiwiP4nIRBGpVZG6PYDxaS/Gp/0Yp/biWZ+evPEv5RxozlA0xqe9GJ/2Y5za\nSzB9BivJ7AAaFZpvaC0rMyJSFZ+cKao6A0BVf1bVPFXNBybgO7UMZ4xPezE+7cc4tRfP+gxWklkK\npIhIsohEAX2BmWWtxBpo7l0gQ1VfKbQ8odBqvYHVFYw31DE+7cX4tB/j1F486zOQ1y9XGFXNFZHB\n+Ia6rgJMVNX0clR1CXArsEpEVljLHgNuEpGTBpqreNShi/FpL8an/Rin9uJln+aJf4PBYDA4hidv\n/BsMBoPBG5gkYzAYDAbHMEnGYDAYDI5hkozBYDAYHMMkGYPBYDA4hkkyBoPBYHAMk2QMBoPB4Bgm\nyRgMBoPBMUySMRgMBoNjeD7JiEh2GdZ9V0RWWsNZf2qNRGooRBl9TrHe1LfaGh68qpOxeRXj1F6M\nT3tx2mdIJhkRqeJQ1UNU9QJVbQ1sBQY7tJ+QwkGfU4BmQCsgBhjg0H5CDuPUXoxPewkln0FPMiKS\nJCJrrYyYYZ1RVBeRTBF5UUSWAzeIyLkiMldElonIQhFpZm2fLCKLRWSViDxXln1b700oGIk0Bt9g\ncJ7GZZ9z1AJYgm/4cc9jnNqL8WkvnvPpxDudSypAEr4/7pdY8xOBYfhG/ny00HrfAinWdEfgn9b0\nTOA2a/o+IFt/e1/1imJKi0L1vgf8DCwAqgf7+4ebT2vdqsBy4FK3fRinoVeMz8rts6JftjuwDtgI\nDC+DoK2F5q8EvrAEJVrLagLHTvmSGdZn+4Gq1vQZBYLKGHcV4B/AHW43mIo6DRGfE4DX3HYXZm00\nJJ0an8ZnWX2W+30y1jW/vwNX4Xvf9FIRmamqawLY/NTLVAXzR6x/I4CDqtomwO0RkVhgYTHr9ysc\nl6rmichU4FF8ZzYhQQWcuuZTRJ4CahOC7/PwahsNVafGp71UGp8VyMCdgXmF5kcAIwLMwgp0tubf\nAR7Gl4XjC623CLjBmhbgAmt6JnCLNX0PAWZhq47zCk2PAca4/Uumok7d8mmtP8CqN8Ztd+HQRkPd\nqfFpfJbHZ7lfWiYifYDuqjrAmr8V6Kiqg09ZbyAwEKBGjRrtkpOT2bBhAzVq1ODo0aNER0eTnJxM\neno6zZs3JzLSd3KVk5PD1q1bOXHiBKpKrVq1qF+/Pjk5OWzevJm8vDzi4uLYs2cPbdu2BWDZsmX7\nVLV2MfFG4MvSZ1jCVwL3qNUZIBQIxGmo+LRiyQW2AFnWohmq+oz9ZspHeXw2a9aMnJwcR5yW5tOK\nJ2Sdhtoxn5mZyb59+6SUmI1Pl495x1+/rKrjgfEA7du3108//ZTU1FRWr7b/ldwisqWEOPLxvXrU\n04SKTyuWoLy+20lO9ZmWlkZmZqYjTkvzacUTdk6daqPt27cPJBbjsww4ccxXpAvzDqBRofmG1jJD\n+bHd6YcffkhERIS/VDJMG7UX49NeguJz+fLlPP3001x77bU89thjHDt2zO5dlEwFridGApuAZCAK\n3+Wn80vapl27duokQFp5v08olLI6DcTnFVdcoZGRkf5ifJbf588//6zTpk3T48ePa3Z2dplcVkaf\ngTitCFbdrnspb3Ha55IlS/Saa67R6OhoFREVEQW0b9++mpOTU+Q2TrTRcv+0VdVcfE/MzwMygOmq\nml7e+ori+PHjjBkzhtjYWGJiYvj+++8B2LJlC88//zz5+fl27s51guG0MmG3zzp16jBw4ED69OnD\n5ZdfblOU3sFOnzt27GD48OEsXryYX3755aSyfv16hg8fTl5enp3hhxxOHu+LFi2iR48ezJo1i8TE\nRN544w1eeuklAKZNm8bcuXPt2E1AVOh6parOAebYFMtJ5Ofn89BDD5GRkcFbb73Fv//9b84991wO\nHDhAt27dOHbsGAMGDKB27RLvo3oOJ51WRuz22alTJ2bPnm1XdZ7DLp8NGzZERBg9enSx6/Tv35/m\nzZtXdFchjRPH++rVq0lNTeXgwYOMHDmSESNGUL16dZYuXWrnbgImJG+KHT58mNtvv5169erx/PPP\n06lTJ8455xzi4+Pp2rUrx44dY+7cuWGXYAwGg6GiPPLIIxw8eJCHH36YZ5991r/8lltuAaBDhw50\n6dIlaPGE5J3guXPnsnPnTp544gk6deoEwKWXXsrChQvJzc3l7bffpmXLli5H6Q1Ulby8PH+59NJL\n2b17t9theZaLLrrIP/2Pf/zDxUi8zZw5c+jevTu33347t956Ky1atDhtnRo1argQmffZvXs3qkrv\n3r39y8aPH8/69etJSUlh+vTpnHXWWUGLJ+TOZB555BG+++47Fi5cSFRUlH/5vffey7hx45g2bRqp\nqakuRugtRIQqVX4bkPWHH35g9OjRvPLKKy5G5V2effZZ8vLyePHFFxk8eDA5OTkMGTLE7bA8xx/+\n8Af+8Ic/+Oezs7OpV68eR48eBaBatWo0btzYrfA8TVJSEitXruTmm29GVdm6dSsAXbt25YILLjjp\n70EwCLkzmY8//pgFCxaclGB++eUX3n77bYYOHUqfPn1cjM57FNzsK8yXX37pQiThw5VXXkmTJk0A\n39nM/v37XY7I+9SsWZPOnTv75wufMRrKRkpKCuDrILVlyxaio6O56qqr6NixI6NHj6ZBgwZBjSek\nkkxubi7PPfcc1atXB3w3/+fPn0+vXr147bXXivyDaSiZs88+2+0Qwo5u3brx3XffAbBp0yY+/PBD\nlyPyJrm5uezevZuhQ4cydOhQv1OAu+66i927dxdbsrKySqi5cvO73/2O6Oho//yvv/5KTk4Oo0eP\nPml5sAipy2WrVq0iNTWV7du3k5mZyZQpU3j77beJioo6qQEaAic+Pp5rrrmGL774AvjtuShDxSjc\n6WTu3Lk8+OCDLkbjTXbu3Enr1q05fPj0kZ369+9f7HZxcXH8/ve/dzI0T9O4cWOuu+46pkyZAsCZ\nZ57JjBkzXIsnpJJMwWndypUrSUpK4swzzyQ6OppWrVq5HZpn2bdvH7NmzfJfh83Ly8P3zjZDRSlI\n1iZpl4/GjRszf/58br31VtatW1fq+k2bNqV27dq89dZbtG7dOqBhZSojnTp1IisrizZt2hAVFcXS\npUtp06aN/95MsAmpy2XNmzenRo0aPPPMMyxfvpzu3bsDBLW7ncEQKLfffjsiQnZ2wK9IN5xC+/bt\nefLJJ09aFhsbS926dbn22muZN28e33333UmldevWLkUb+owfP56srCwuueQS3nvvPSZNmkRsbCzb\nt293LaaQOpMB+P7779m2bRt16tShZs2abNq0iYSEBLfDMhhO47333iM/P9/ck7GZN998k9tuu83t\nMDzF2rVradGiRZGXw//3v/9Rp04dFi1axMUXXxz02ELqTAZ8l3OuvPJKmjVrxpo1a0yCqSA1a9ak\nZcuW/udkVJXNmze7HVbYcPfdd7sdQlhRt25d/0ODhsC58847UVV/z7LCFDw3Exsb60JkIZhk3nzz\nTXbt2sVPP/1EvXr13A7H88THx/P5559Tr149qlSpctpzM4aKUb16dXNPxkays7NdG/7Eq3z77bek\npaUhIic9IPzrr78ybtw4unXrhoi4dm875JLMyJEjGT58uNthhBVJSUmndV18//33XYom/DAdKezj\nyJEjAXUCMPxGdnY2ubm5gM/fl19+Sd++fbn44ou5++672bNnDzfeeKNr8ZWaZESkkYgsEJE1IpIu\nIg9ay0eJyA4RWWGVHhUNZvLkyfTq1Susk0wwfRamWbNmNGvWzD8/efJkO6t3Dbd8hjPGqb047fP8\n88/3P2DZq1cvevfuzfTp01mxYgWXXXYZr7/+uqvHeyA3/nOBh1V1uYjEAstE5Bvrs1dVdYxdwbz0\n0kssWbLE//rQMCVoPgszceJEjhw5wujRo5k4cSIDBgxwYjdu4IrPU9m/f384PfjqmtP4+Hh69Ai7\n3OWoz/POO4/Zs2fTvXt3du/eTWJiIsOHD6dVq1au3Og/lVL/mqvqLmCXNZ0lIhmA7eMS7N69mxEj\nRhATE2N31SFFsHyeSsH9rXHjxjFu3Dindxc03PJZwAUXXBB27z0JttPrr7+ezZs3k5WVxe9+9zvi\n4+Od2pUrBMPnBRdcwK5du+ys0jbKdE9GRJKAtsB/rUWDReQnEZkoIrWK2WagiKSJSNrevXuLrbte\nvXr069evLOF4Hid9VkaMT/sJhtOoqCgee+wxnn/+ebp162ZX6CFJZWyjAScZEakJfAY8pKqHgbHA\nuUAbfFn65aK2U9XxqtpeVdub97/8hvFpL8an/Rin9lJZfUog3S9FpCowG5inqqeNEW9l59mqWuJL\nXkQkC7C760g8sM+aTlTVkP9fsNHnXuAIv31/uyhwanzag6d8gmeOeePTHhz9G1rqPRnx9c98F8go\nLEdEEqxrjQC9gdUB7G+dqto64JCIpNldp5PY6VNVazvx/b3k1Pi0H3PM20tl9xlIN65LgFuBVSKy\nwlr2GHCTiLQBFMgEBjkSYfhhfNqL8Wk/xqm9VGqfgfQu+x4o6mmzOfaHE/4Yn/ZifNqPcWovld1n\nsJ/4H++ROr2EcWovxqe9GJ/24jmfAd34NxgMBoOhPITc2GUGg8FgCB9MkjEYDAaDYwQtyYhIdxFZ\nJyIbRaRcI2Cagft+w/i0F+PTfoxTe/Gsz4I3qTlZgCrA/4BzgChgJdCiHPUkABda07HAeqAFMAoY\nFozvEgrF+DQ+Q70Yp8ZnQQnWmUwHYKOqblLV48BUoGdZK1HVXaq63JrOAoI6GGIIYXzai/FpP8ap\nvXjWZ7CSTANgW6H57VTwi0k5BpoLI4xPezE+7cc4tRfP+vTkjX8p50BzhqIxPu3F+LQf49Regukz\nWElmB9Co0HxDa1mZEd9Ac58BU1R1BoCq/qyqeaqaD0zAd2oZzhif9mJ82o9xai+e9RmsJLMUSBGR\nZBGJAvoCM8taSUkDzRVaLdCB5ryM8Wkvxqf9GKf24lmfQXnPsarmishgYB6+XhITVTW9HFVV6oHm\nCjA+7cX4tB/j1F687NMMK2MwGAwGx/DkjX+DwWAweAOTZAwGg8HgGCbJGAwGg8ExTJIxGAwGg2OY\nJGMwGAwGxzBJxmAwGAyOYZKMwWAwGBzDJBmDwWAwOIbnk4yIZJdh3SnWS39WWyONVnUyNi9ifNpP\nGZ1eKSLLLafvi0hQRuXwEmX0ubDQi7h2isgXTsbmRZw+5kMyyYhIFYeqngI0A1oBMcAAh/YTUhif\n9uOEUxGJAN4H+qpqS2AL0N/u/YQiTrVRVb1UVduoahtgMTDDif2EGqF0zAc9yYhIkoistTJihoh8\nKiLVRSRTRF4UkeXADSJyrojMFZFl1q+RZtb2ySKyWERWichzZdm3qs5RC2AJvpFMPY3xaT8uOj0b\nOK6q6635b4Drbf56QcfNNloohjOAKwHPn8l47ph34nWbJRUgCd8gbJdY8xOBYfgGZXu00HrfAinW\ndEfgn9b0TOA2a/o+IFt/e5XoimJKi1NiqAosBy4N9vc3PkO/uOUUEHxnL+2t9V8HVrntw6s+T4nh\nNuBTt12Ekc+Aj/mKftnuwDpgIzC8DIK2Fpov+HWRCSRay2oCx075khnWZ/uBqtb0GQWCyhj3BOA1\ntxuLHU6Nz/Bqo0BnYCG+X4nPASvcduhln4X2+RVwvdv+wshnwMd8uW8qWtf8/g5che9VoEtFZKaq\nrglg81OHfi6YP2L9GwEcVN911EC2R0Ri8R2cRdGvIC4ReQqoTQgODV4Bp8ZnEXixjarqYuBSa/3f\nA00CiDUoeNGntV48vpdw9Q4gzqDhYZ9lO+YrkIE7A/MKzY8ARgSYhRXobM2/AzyMLwvHF1pvEXCD\nNS3ABdb0TOAWa/oeyvYrcYBVb4zbv2Dscmp8hl0brWP9Ww3f5Y4r3fboZZ/WNncD77vtLxx8lueY\nL/f7ZESkD9BdVQdY87cCHVV1cHHbxMfHa0JCAhs2bKBGjRocPXqU6OhokpOTSU9Pp3nz5kRG+k6u\ncnJy2Lp1KydOnEBVqVWrFvXr1ycnJ4fNmzeTl5dHXFwce/bsoW3btgAsW7Zsn6rWLiHmXHzXvLOs\nRTNU9ZlyCXCAsjo1PkumPD6TkpLIyclxxGlpPq0YXwJS8f0SHauqr9nppCKE2jGfmZnJvn37JIC4\n/wW8oKpzbRFhE6HmE5w55h3vgy8iA4GBAI0bN2bWrFmkpqayerX9b0sVkS0lfa6qnn/mwPi0l1N9\npqWlkZmZ6YjT0nwCqOojwCO27jjIBKuNtm/fPqD1VPVyW3ccZLx+zFekC/MOoFGh+YbWslODGq+q\n7VW1fe3aJf6IMwTg1PgsE8anvZhj3l4qhc+KJJmlQIrV5zoK6IvvWl+JJCUlOZKBw4QyOzU+S8Tx\nNvr2228jUuoVm3DBHPP2Uil8lvtyh6rmishgYB5QBZioqum2RVYJMU7txWmf//jHPxg8uNjL52GH\naZ/2Ull8VuiauqrOAebYFAsAX3/9NR9++CGbN2/mnHPOYdOmTdx8883cfffddu4mZLHTabt27Wja\ntCk9evTgsssuA6B69eqcffbZdlTvCZxoowCLFi3iwQcfpFq1anz44Yd2Vx+yOOWzgIyMDF577TWm\nT5/OJ598Qrdu3ZzaVUhgt8+dO3fy/vvvA/DNN9+wYMGC086069Spw/z582nZsqVduy2ZYHbZa9eu\nnZbEjh07VEQUUBHxF1+YpQOkBfP7uF1K8ykiGhERoREREf7pxMREXbVqlfFZDp8FpKena6NGjTQy\nMlLfeOONgLZRrXw+tQxOVVXvuusuFRFt0qSJ9uzZU6OjozU9Pb3Y9a26Xf+OoeDz8ccf1zPPPFNj\nY2P9x3zh4/7UMnny5CLrcaKNhtQAmbVq1QIgOjqa+++/n5deesn/2d69e90Ky7M8+eSTpy3btm0b\njz/+uAvRhAeZmZlcfd7Ffv0AAAmvSURBVPXV7Nixg5dffpn777/f7ZA8z8GDB+natSvvvPMOr7/+\nOqtXr+aLL76gcePGrFixwu3wQp5PPvmEMWPGkJWVxZEjR076rH///vTv358uXbr4l6WkpHD99cEb\nEi+kuqDGxMRw2223kZqaSp8+fdi9ezePPOLrzfnrr7+6HJ33GDlyJHPmzKFdu3aMGzfOv3z27Nku\nRuVthg4dyo4dOxgyZAgPPPCAf7mqVqYOALZyww03sHDhQr788kv++Mc/UqWKbwDhOnXq8P3339Ov\nXz+XIwxtateuTXR0NCdOnABgzZrfBgxo2rQpAIcOHeKss84C4JxzzmHfvn00btw4KPGF1JkMwKRJ\nk+jTpw/ASde6jx075lZIniUyMpIlS5YwduxY8vPzycvLY+rUqW6H5Vm2bNnCxo0bWbNmDS+//LJ/\n+datW2nVqpX5IVQOpk+fTrt27cjNzeXaa6/1JxiA77//nk6dOrkYnTe4/PLLmTt3rj+h/PnPfyY7\nO5umTZuyc+dOUlNTSUhI4IwzzuC///0vc+bMCVqCgRA7kzmVwhm5SZOQGcLJUEl54YUX+POf/0yz\nZs38y06cOMHjjz9O3bp1iY6OdjE6b/LII4+wevXqk5IL4L/s06hRo6I2M5xCp06duOeee3jooYdY\nvHgxl1xyCT169GDDhg3+v6OTJk0K+AFWOwnZJDN//nxmzKgU7xcyeICNGzfy3nvvnXZGvW3bNj78\n8EPeeOMNlyLzLrNnz2b79u3Exsae9tnx48dJSUmhc+fOLkTmTfr06cO0adNYvHgxJ06c4Msvv/Rf\nxq1RowY33nijK3GF3OWyAubNm8ehQ4fwdXgw2MV3332HqrJt2za2bdtGbm6u2yF5glmzZpGTk3Pa\n8k8++QTw3VcwlI2//e1v5Ofnn7Y8JyeH66+/nnfffdecHZaBhIQEPvvsM6Kiok77rGfPni5E5CMk\nk0xWVhZLlixBRBARhg4d6nZInmX//v0sXryYG2+8kT/96U9MmzYNESEpKYmkpCTuu+8+t0P0BFdf\nfTWRkZFcf/31pKWlAbBhwwaee873YsE6deq4GZ4n2bVr12nLli9fTqdOnViwYAGXXnqpC1F5m7p1\n6/Luu++etvyjjz5yIRofIXm57N1332Xhwt9eazBy5EgXo/E2Xbp0Yf369af1foqPj2fQoEH8+c9/\ndjE679CiRQueeuopnnjiCRYsWECHDh1YtGgR2dnZREREEBERkr/XQppx48bxxz/+kREjRhATE8NX\nX33FihUrTAeKCvLyyy/7rwCFxJWgUHiQqIgHghTQuLg43blzZ0DbOPUgUSiXQHx269ZNX3nlFb3s\nsstURLRNmzZ6+PBh47OcPnNycvTo0aO6dOlSBbRGjRqamZlpfFbAaWGefvppBbRv376lrmsexjyd\n5cuXa0REhJ555pmalZWlqqozZszQ2NhYHTduXKlOnWijIffza/78+QCICNdccw0JCQkuR+Rtvvnm\nG4YMGUL9+vUREZo2bVrkjVZDYERFRRETE0N2djbguw6emJjoclThRzC72IYTBQ+wDx06lJo1awLQ\nu3dv4uLiGDZsGP/5z3+CHlPIJZnCT1APGzbMxUjCi4LnY/r27etyJOHB6NGjAd+9GoN9bNy4EYAr\nrrjC5Ui8x9KlS/nqq68A3+Xwwtx9990cOXKEo0ePBj2uUpOMiDQSkQUiskZE0kXkQWv5KBHZISIr\nrNLDjoDWrVtnRzUhS7B9FqZdu3b06GF7ta7ihs8VK1b4z7jDEbfaaF5eHkuXLiUhIYFLLrnEzqpd\nxc1jvoCGDRs6VXWpBHLjPxd4WFWXi0gssExEvrE+e1VVx9gZkO+yYIjcsHKGoPoEWLZsGarKo48+\nWmT3Ro8TdJ/Z2dmcOHGCmjVrcvPNN9tdfSgQdKcAY8eOZe3atdx5553hdkk3KD4vuugiLrroIr79\n9tuTxnpct24dkyZNombNmlSvXt2OXZWJUpOMqu4CdlnTWSKSATRwKiARQVW59tpradGihVO7cY1g\n+wTfGUy4jqvlhs86deoQExNDu3btwvJhQTecAuzZswfwXSpLS0tz5el0Jwimz+uvv55vv/2WF154\ngc2bN6OqTJ8+nePHj3Pddde5MkxPmbowi0gS0Bb4L3AJMFhEbgPS8GXqAxUNaMeOHbz++us0adKE\nyMiQ7GFtG8HwWcBdd91lV1UhS7B8NmnSxJVr224QzDYaERHB+eefT1JSUtgkmFNx2uegQYNo0KAB\ny5cv5+mnn0ZVGTVqFBdeeCGpqakVDb98BNoNDagJLAOus+br4nubWwTwV3xvdStqu4H4BKY1bty4\n1C50FQEPdRE1Po3PUC+h7tRrXZhD3aeqM200oN5lIlIV+AyYoqozrOT0s6rmqWo+MAHoUNS2qjpe\nVduravvatWsHsruwx/i0F+PTfoxTe6nMPsWXvEpYwXcx/33gF1V9qNDyBPVda0REhgAdVbXE/rEi\nkgXY3X0sHthnTSeqakj/L9jscy9whN++v10UODU+7cEzPsFTx7zxaQ+O/g0NJMl0ARYCq4CC0ewe\nA24C2uB7Oj8TGFQgrIS60lTV1outTtTpJHb6tOqr1E6NT/sxx7y9VHafgfQu+x4oqmvSHPvDCX+M\nT3sxPu3HOLWXyu4z5J74NxgMBkP4EOwkM94jdXoJ49RejM//b+/+VaOIogCMf6dJpYVliBYqNikt\n0vgCdmKnhW9gYWEhVr6Aaa1MbW3nMwjiX0SJNirptRPhWMwIqwiyO2du5sr3g4HdIVxmv+aQTOZu\nLXvW6q7nP+/JSJK0Kf9cJkmaTbMhExGXI+JdRBxGxJ0N1zj2jeaWwp617FnPprW67Vn9dOffDoan\nWj8A54At4AWwu8E628DF8fVJ4D2wC9wDbrf4LEs47GnPpR82teevo9VvMnvAYWZ+zMzvwCPgyrqL\nZOZRZj4bX38Dmmzct0D2rGXPejat1W3PVkNmB/i08v4zEz/YHxvNwbDR3MuIOIiIU1PW7oA9a9mz\nnk1rdduzyxv/EXGCYR+gW5n5FXgAnGd4evYIuH+Ml9cde9ayZz2b1mrZs9WQ+QKcWXl/ejy3tpiw\n0dx/xJ617FnPprW67dlqyDwFLkTE2YjYAq4Bj9ddZNxo7iHwNjP3V85vr/zYVeD1xOtdOnvWsmc9\nm9bqtmeTbwXLzB8RcRN4wvBfEgeZ+WaDpS4BN4BXEfF8PHcXuB4Rv200N/2ql8uetexZz6a1eu7p\nE/+SpNl0eeNfktQHh4wkaTYOGUnSbBwykqTZOGQkSbNxyEiSZuOQkSTNxiEjSZrNTzd5UDLXWbsM\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}